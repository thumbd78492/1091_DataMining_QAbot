{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BOT_build_data.ipynb","provenance":[],"authorship_tag":"ABX9TyM7rr7WTHSiDB0HMJgEsycn"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6y8IvHWcM1rE"},"source":["# 環境設置"]},{"cell_type":"code","metadata":{"id":"QZPZVBC1Me92","executionInfo":{"status":"ok","timestamp":1604576415023,"user_tz":-480,"elapsed":53224,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}},"outputId":"e802582c-f421-405e-834f-fb88d24860d6","colab":{"base_uri":"https://localhost:8080/"}},"source":["# 環境初始化 (大約三至五分鐘)\n","! wget -O init_env.sh https://www.dropbox.com/s/6bnwn8u2hz19s59/init_env.sh && \\\n","bash init_env.sh\n","\n","import os, sys\n","os.environ['SPARK_HOME'] = \"/usr/local/spark\"\n","# os.environ['PYSPARK_PYTHON'] = \"/usr/bin/python\"\n","os.environ['PYSPARK_PYTHON'] = \"/usr/local/bin/python\"\n","sys.path.append(\"/usr/local/spark/python/\")\n","sys.path.append(\"/usr/local/spark/python/lib/pyspark.zip\")\n","sys.path.append(\"/usr/local/spark/python/lib/py4j-0.10.4-src.zip\")\n","\n","from pyspark import SparkContext\n","from pyspark import SparkConf\n","sc = SparkContext()\n","\n","from google.colab import drive\n","# 將自己的雲端硬碟掛載上去\n","drive.mount('/content/gdrive')\n","file_dir = \"/content/gdrive/My Drive/DataMining/DataWithPTT/\"\n","\n","!ls -l /content/gdrive/My\\ Drive/DataMining/DataWithPTT"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2020-11-05 11:39:22--  https://www.dropbox.com/s/6bnwn8u2hz19s59/init_env.sh\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.1, 2620:100:6016:1::a27d:101\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.1|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/6bnwn8u2hz19s59/init_env.sh [following]\n","--2020-11-05 11:39:22--  https://www.dropbox.com/s/raw/6bnwn8u2hz19s59/init_env.sh\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uce3ec6b4ed20eb499823a7db801.dl.dropboxusercontent.com/cd/0/inline/BCnObCgzzdPo3YGrpQmp8o_TlcznhztO_YNH4Pvdpwju-tFEvzjGrz6Kl8vdok0_rNK-P64DxudXOGMX_3u-sLR-eR5b9ykf0781_-181flfJw/file# [following]\n","--2020-11-05 11:39:22--  https://uce3ec6b4ed20eb499823a7db801.dl.dropboxusercontent.com/cd/0/inline/BCnObCgzzdPo3YGrpQmp8o_TlcznhztO_YNH4Pvdpwju-tFEvzjGrz6Kl8vdok0_rNK-P64DxudXOGMX_3u-sLR-eR5b9ykf0781_-181flfJw/file\n","Resolving uce3ec6b4ed20eb499823a7db801.dl.dropboxusercontent.com (uce3ec6b4ed20eb499823a7db801.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n","Connecting to uce3ec6b4ed20eb499823a7db801.dl.dropboxusercontent.com (uce3ec6b4ed20eb499823a7db801.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 336 [text/plain]\n","Saving to: ‘init_env.sh’\n","\n","init_env.sh         100%[===================>]     336  --.-KB/s    in 0s      \n","\n","2020-11-05 11:39:23 (42.7 MB/s) - ‘init_env.sh’ saved [336/336]\n","\n","--2020-11-05 11:39:23--  https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz\n","Resolving d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)... 99.86.33.103, 99.86.33.32, 99.86.33.197, ...\n","Connecting to d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)|99.86.33.103|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 203728858 (194M) [application/x-tar]\n","Saving to: ‘spark-2.2.0-bin-hadoop2.7.tgz’\n","\n","spark-2.2.0-bin-had 100%[===================>] 194.29M  71.1MB/s    in 2.7s    \n","\n","2020-11-05 11:39:25 (71.1 MB/s) - ‘spark-2.2.0-bin-hadoop2.7.tgz’ saved [203728858/203728858]\n","\n","spark-2.2.0-bin-hadoop2.7/\n","spark-2.2.0-bin-hadoop2.7/NOTICE\n","spark-2.2.0-bin-hadoop2.7/jars/\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-common-1.8.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-net-2.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/oro-2.0.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/antlr-runtime-3.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-1.8.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-sketch_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/stream-2.7.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/kryo-shaded-3.0.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/metrics-jvm-3.1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/RoaringBitmap-0.5.11.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/activation-1.1.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jta-1.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jets3t-0.9.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-compress-1.4.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-1.7.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-format-2.3.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/avro-1.7.7.jar\n","spark-2.2.0-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jline-2.12.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/metrics-core-3.1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/java-xmlbuilder-1.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jsp-api-2.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/xmlenc-0.52.jar\n","spark-2.2.0-bin-hadoop2.7/jars/xbean-asm5-shaded-4.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n","spark-2.2.0-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javax.inject-1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-sql_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/json4s-jackson_2.11-3.2.11.jar\n","spark-2.2.0-bin-hadoop2.7/jars/json4s-ast_2.11-3.2.11.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-codec-1.10.jar\n","spark-2.2.0-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/minlog-1.3.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javolution-5.5.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-common-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-graphx_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n","spark-2.2.0-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n","spark-2.2.0-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-network-common_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/pmml-schema-1.2.15.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/joda-time-2.9.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/curator-recipes-2.6.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-server-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n","spark-2.2.0-bin-hadoop2.7/jars/httpclient-4.5.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/snappy-0.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n","spark-2.2.0-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-tags_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-databind-2.6.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/curator-client-2.6.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/paranamer-2.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/opencsv-2.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/json4s-core_2.11-3.2.11.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-digester-1.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jsr305-1.3.9.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-repl_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jetty-6.1.26.jar\n","spark-2.2.0-bin-hadoop2.7/jars/pyrolite-4.13.jar\n","spark-2.2.0-bin-hadoop2.7/jars/log4j-1.2.17.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scala-xml_2.11-1.0.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar\n","spark-2.2.0-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jtransforms-2.4.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar\n","spark-2.2.0-bin-hadoop2.7/jars/guice-3.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/gson-2.2.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/httpcore-4.4.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-yarn_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-hive_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-lang-2.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/netty-all-4.0.43.Final.jar\n","spark-2.2.0-bin-hadoop2.7/jars/curator-framework-2.6.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-io-2.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/avro-ipc-1.7.7.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/mesos-1.0.0-shaded-protobuf.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-annotations-2.6.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-jackson-1.8.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-module-paranamer-2.6.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/aopalliance-1.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/janino-3.0.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/antlr4-runtime-4.5.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jpam-1.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-lang3-3.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar\n","spark-2.2.0-bin-hadoop2.7/jars/bcprov-jdk15on-1.51.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-launcher_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/metrics-graphite-3.1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n","spark-2.2.0-bin-hadoop2.7/jars/lz4-1.3.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/core-1.1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-mesos_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/antlr-2.7.7.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/mx4j-3.0.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-core-1.8.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/libthrift-0.9.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-encoding-1.8.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-mllib_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n","spark-2.2.0-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n","spark-2.2.0-bin-hadoop2.7/jars/mail-1.4.7.jar\n","spark-2.2.0-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/metrics-json-3.1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scalap-2.11.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/zookeeper-3.4.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/ivy-2.4.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/py4j-0.10.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar\n","spark-2.2.0-bin-hadoop2.7/jars/pmml-model-1.2.15.jar\n","spark-2.2.0-bin-hadoop2.7/jars/parquet-column-1.8.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar\n","spark-2.2.0-bin-hadoop2.7/jars/breeze_2.11-0.13.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-cli-1.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/chill-java-0.8.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/avro-mapred-1.7.7-hadoop2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/snappy-java-1.1.2.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/base64-2.3.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-compiler-3.0.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar\n","spark-2.2.0-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/objenesis-2.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jersey-client-2.22.2.jar\n","spark-2.2.0-bin-hadoop2.7/jars/ST4-4.0.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/univocity-parsers-2.2.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.0.4.jar\n","spark-2.2.0-bin-hadoop2.7/jars/jackson-core-2.6.5.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/xz-1.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scala-reflect-2.11.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scala-compiler-2.11.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar\n","spark-2.2.0-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-core_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/guava-14.0.1.jar\n","spark-2.2.0-bin-hadoop2.7/jars/netty-3.9.9.Final.jar\n","spark-2.2.0-bin-hadoop2.7/jars/spark-streaming_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/chill_2.11-0.8.0.jar\n","spark-2.2.0-bin-hadoop2.7/jars/scala-library-2.11.8.jar\n","spark-2.2.0-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar\n","spark-2.2.0-bin-hadoop2.7/python/\n","spark-2.2.0-bin-hadoop2.7/python/run-tests.py\n","spark-2.2.0-bin-hadoop2.7/python/test_support/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/hello/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/hello/sub_hello/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n","spark-2.2.0-bin-hadoop2.7/python/test_support/hello/hello.txt\n","spark-2.2.0-bin-hadoop2.7/python/test_support/userlibrary.py\n","spark-2.2.0-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people.json\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people1.json\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/streaming/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/text-test.txt\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people_array.json\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n","spark-2.2.0-bin-hadoop2.7/python/test_support/sql/ages.csv\n","spark-2.2.0-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n","spark-2.2.0-bin-hadoop2.7/python/pylintrc\n","spark-2.2.0-bin-hadoop2.7/python/docs/\n","spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.ml.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.streaming.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/conf.py\n","spark-2.2.0-bin-hadoop2.7/python/docs/_templates/\n","spark-2.2.0-bin-hadoop2.7/python/docs/_templates/layout.html\n","spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/make.bat\n","spark-2.2.0-bin-hadoop2.7/python/docs/epytext.py\n","spark-2.2.0-bin-hadoop2.7/python/docs/make2.bat\n","spark-2.2.0-bin-hadoop2.7/python/docs/index.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/_static/\n","spark-2.2.0-bin-hadoop2.7/python/docs/_static/pyspark.js\n","spark-2.2.0-bin-hadoop2.7/python/docs/_static/pyspark.css\n","spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.sql.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.mllib.rst\n","spark-2.2.0-bin-hadoop2.7/python/docs/Makefile\n","spark-2.2.0-bin-hadoop2.7/python/.gitignore\n","spark-2.2.0-bin-hadoop2.7/python/MANIFEST.in\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/status.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/version.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/conf.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/base.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/util.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/classification.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/regression.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/tests.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/tuning.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/common.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/stat.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/linalg/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/feature.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/clustering.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/fpm.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/statcounter.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/profiler.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/serializers.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/traceback_utils.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/shell.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/conf.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/session.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/window.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/tests.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/utils.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/group.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/types.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/catalog.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/context.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/column.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/streaming.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/functions.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/taskcontext.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/util.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/python/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/python/pyspark/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/python/pyspark/shell.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/daemon.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/tests.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/resultiterable.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/heapq3.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/broadcast.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/shuffle.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/cloudpickle.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/accumulators.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/java_gateway.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/util.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/listener.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/tests.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/flume.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kafka.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/context.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/context.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/storagelevel.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/join.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tree.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/util.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/classification.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/regression.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tests.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/common.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/feature.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/random.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/rdd.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/rddsampler.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/worker.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/files.py\n","spark-2.2.0-bin-hadoop2.7/python/pyspark/find_spark_home.py\n","spark-2.2.0-bin-hadoop2.7/python/setup.cfg\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/SOURCES.txt\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/requires.txt\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/PKG-INFO\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/dependency_links.txt\n","spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/top_level.txt\n","spark-2.2.0-bin-hadoop2.7/python/run-tests\n","spark-2.2.0-bin-hadoop2.7/python/dist/\n","spark-2.2.0-bin-hadoop2.7/python/setup.py\n","spark-2.2.0-bin-hadoop2.7/python/lib/\n","spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip\n","spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip\n","spark-2.2.0-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n","spark-2.2.0-bin-hadoop2.7/python/README.md\n","spark-2.2.0-bin-hadoop2.7/RELEASE\n","spark-2.2.0-bin-hadoop2.7/sbin/\n","spark-2.2.0-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/spark-daemon.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-slaves.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-thriftserver.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-shuffle-service.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-history-server.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/spark-config.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-history-server.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-thriftserver.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-shuffle-service.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/spark-daemons.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-all.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-master.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-slave.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-slave.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-slaves.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/stop-all.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/slaves.sh\n","spark-2.2.0-bin-hadoop2.7/sbin/start-master.sh\n","spark-2.2.0-bin-hadoop2.7/examples/\n","spark-2.2.0-bin-hadoop2.7/examples/src/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/als.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/dataframe.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/pagerank.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/pi.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/kafka_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/direct_kafka_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/flume_wordcount.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/kmeans.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/als.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sort.py\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaFlumeEventCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaKafkaWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRegressionMetricsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLinearRegressionWithSGDExample.java\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/FlumePollingEventCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/FlumeEventCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/KafkaWordCount.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RegressionMetricsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegressionWithSGDExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegression.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/people.json\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/employees.json\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/people.txt\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/users.parquet\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/user.avsc\n","spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/users.avro\n","spark-2.2.0-bin-hadoop2.7/examples/jars/\n","spark-2.2.0-bin-hadoop2.7/examples/jars/scopt_2.11-3.3.0.jar\n","spark-2.2.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.2.0.jar\n","spark-2.2.0-bin-hadoop2.7/data/\n","spark-2.2.0-bin-hadoop2.7/data/graphx/\n","spark-2.2.0-bin-hadoop2.7/data/graphx/followers.txt\n","spark-2.2.0-bin-hadoop2.7/data/graphx/users.txt\n","spark-2.2.0-bin-hadoop2.7/data/streaming/\n","spark-2.2.0-bin-hadoop2.7/data/streaming/AFINN-111.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/\n","spark-2.2.0-bin-hadoop2.7/data/mllib/pagerank_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/kmeans_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/pic_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/als/\n","spark-2.2.0-bin-hadoop2.7/data/mllib/als/test.data\n","spark-2.2.0-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/ridge-data/\n","spark-2.2.0-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n","spark-2.2.0-bin-hadoop2.7/data/mllib/gmm_data.txt\n","spark-2.2.0-bin-hadoop2.7/R/\n","spark-2.2.0-bin-hadoop2.7/R/lib/\n","spark-2.2.0-bin-hadoop2.7/R/lib/sparkr.zip\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/groupBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/covar_pop.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sampleBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sql.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/year.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tan.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GBTRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/last_day.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sign.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hint.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/randn.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/orderBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/otherwise.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/AFTSurvivalRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hashCode.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bin.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dim.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.svmLinear.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/minute.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createExternalTable-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/distinct.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.conf.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.jobj.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/md5.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cbrt.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.ml.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dapplyCollect.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/acos.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tables.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/NaiveBayesModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sum.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/structType.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hex.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isLocal.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.jdbc.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_utc_timestamp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tableNames.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createDataFrame.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isStreaming.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toJSON.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.getSparkFiles.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/except.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LDAModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/months_between.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark_partition_id.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.parquet.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sumDistinct.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/awaitTermination.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/BisectingKMeansModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sha2.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/abs.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_format.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/withColumn.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dayofyear.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sort_array.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/storageLevel.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setCurrentDatabase.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ceil.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/floor.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stddev_pop.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sd.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.structType.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.survreg.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/predict.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/count.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unhex.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/mean.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/instr.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_unixtime.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/saveAsTable.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ltrim.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkRHive.init-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.parquet.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/match.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/is.nan.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.ml.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lag.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.json.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unpersist.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/corr.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.callJStatic.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rint.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LinearSVCModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.gbt.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/RandomForestClassificationModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/persist.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var_pop.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/selectExpr.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crc32.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/expr.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.callJMethod.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/with.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/generateAliasesForIntersectedCols.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GeneralizedLinearRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/IsotonicRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setLogLevel.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftRightUnsigned.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/base64.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/array_contains.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/expm1.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.orc.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.version.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/insertInto.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/SparkDataFrame.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/merge.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dayofmonth.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listDatabases.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/summarize.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/format_number.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_json.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropDuplicates.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cache.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.text.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/approxCountDistinct.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkRSQL.init-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LogisticRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stddev_samp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/pivot.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/showDF.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/between.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/struct.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/subset.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/posexplode.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lit.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftLeft.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/glm.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hypot.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/recoverPartitions.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.session.stop.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/translate.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/drop.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GBTClassificationModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftRight.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/regexp_replace.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/randomSplit.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/length.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rowsBetween.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.jdbc.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/schema.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toRadians.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/filter.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bround.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createOrReplaceTempView.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cancelJobGroup.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/second.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/upper.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/head.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/limit.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/concat_ws.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/when.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/FPGrowthModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/install.spark.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.newJObject.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropTempView.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unbase64.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/soundex.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/structField.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.addFile.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.bisectingKmeans.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cacheTable.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cosh.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.mlp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ntile.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/atan2.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.kstest.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dtypes.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/reverse.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sinh.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.lda.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createTable.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/negate.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/asin.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hash.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rank.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toDegrees.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/columns.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/columnfunctions.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/substring_index.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_timestamp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.naiveBayes.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/atan.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.isoreg.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/factorial.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/countDistinct.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/quarter.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setCheckpointDir.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/least.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.text.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/windowOrderBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dapply.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/coalesce.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/refreshByPath.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cume_dist.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dense_rank.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/freqItems.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/getNumPartitions.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/KMeansModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.session.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/arrange.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.stream.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/encode.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.glm.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isActive.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crossJoin.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rpad.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/uncacheTable.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/size.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/conv.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log10.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/collect.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.stream.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/format_string.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/windowPartitionBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/union.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stopQuery.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropTempTable-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/endsWith.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/startsWith.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nanvl.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/mutate.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/explain.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/R.css\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cov.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var_samp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log2.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/registerTempTable-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lastProgress.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/attach.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/min.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ncol.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/month.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/window.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/partitionBy.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/percent_rank.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listFunctions.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_utc_timestamp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crosstab.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/take.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/exp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_json.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/column.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ifelse.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GaussianMixtureModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.logit.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/show.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setJobGroup.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/KSTest-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/clearJobGroup.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/last.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/printSchema.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rename.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rbind.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/over.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/MultilayerPerceptronClassificationModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/coltypes.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/datediff.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lead.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/summary.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/WindowSpec.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unix_timestamp.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tanh.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listColumns.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_date.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.uiWebUrl.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/max.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.structField.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.als.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/alias.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sha1.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/status.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.orc.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/currentDatabase.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.df.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/round.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.init-deprecated.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/refreshTable.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nrow.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/pmod.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/intersect.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rtrim.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/substr.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/levenshtein.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/monotonically_increasing_id.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/checkpoint.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/decode.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.lapply.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.json.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/RandomForestRegressionModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/trim.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/select.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/regexp_extract.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/as.data.frame.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rangeBetween.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/queryName.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sample.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lower.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/repartition.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/concat.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cast.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_add.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.fpGrowth.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hour.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/initcap.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/explode.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/add_months.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bitwiseNOT.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/approxQuantile.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/kurtosis.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/greatest.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/first.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.df.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rand.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/next_day.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/clearCache.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nafunctions.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/row_number.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lpad.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/skewness.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/gapplyCollect.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/locate.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/avg.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sqrt.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cos.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log1p.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/str.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/StreamingQuery.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ALSModel-class.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ascii.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listTables.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sin.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/histogram.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/weekofyear.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GroupedData.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/fitted.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_sub.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tableToDF.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.getSparkFilesRootDirectory.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/join.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/gapply.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.randomForest.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.kmeans.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.gaussianMixture.html\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/INDEX\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n","spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n","spark-2.2.0-bin-hadoop2.7/licenses/\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scalacheck.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-Mockito.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-junit-interface.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-boto.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-spire.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scala.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-SnapTree.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jline.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-heapq.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jpmml-model.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jbcrypt.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-postgresql.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-DPark.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n","spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n","spark-2.2.0-bin-hadoop2.7/conf/\n","spark-2.2.0-bin-hadoop2.7/conf/fairscheduler.xml.template\n","spark-2.2.0-bin-hadoop2.7/conf/metrics.properties.template\n","spark-2.2.0-bin-hadoop2.7/conf/spark-env.sh.template\n","spark-2.2.0-bin-hadoop2.7/conf/log4j.properties.template\n","spark-2.2.0-bin-hadoop2.7/conf/docker.properties.template\n","spark-2.2.0-bin-hadoop2.7/conf/slaves.template\n","spark-2.2.0-bin-hadoop2.7/conf/spark-defaults.conf.template\n","spark-2.2.0-bin-hadoop2.7/LICENSE\n","spark-2.2.0-bin-hadoop2.7/bin/\n","spark-2.2.0-bin-hadoop2.7/bin/spark-shell\n","spark-2.2.0-bin-hadoop2.7/bin/spark-submit.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-shell2.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/pyspark\n","spark-2.2.0-bin-hadoop2.7/bin/sparkR.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-class2.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/run-example.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-submit2.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-class\n","spark-2.2.0-bin-hadoop2.7/bin/spark-submit\n","spark-2.2.0-bin-hadoop2.7/bin/spark-sql\n","spark-2.2.0-bin-hadoop2.7/bin/find-spark-home\n","spark-2.2.0-bin-hadoop2.7/bin/run-example\n","spark-2.2.0-bin-hadoop2.7/bin/beeline\n","spark-2.2.0-bin-hadoop2.7/bin/pyspark2.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-shell.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/spark-class.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/pyspark.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/sparkR\n","spark-2.2.0-bin-hadoop2.7/bin/beeline.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/sparkR2.cmd\n","spark-2.2.0-bin-hadoop2.7/bin/load-spark-env.sh\n","spark-2.2.0-bin-hadoop2.7/bin/load-spark-env.cmd\n","spark-2.2.0-bin-hadoop2.7/yarn/\n","spark-2.2.0-bin-hadoop2.7/yarn/spark-2.2.0-yarn-shuffle.jar\n","spark-2.2.0-bin-hadoop2.7/README.md\n","環境初始化完畢\n","Mounted at /content/gdrive\n","total 4496026\n","-rw------- 1 root root 1663618524 Nov  4 01:46 jsonJieba-tran.json\n","-rw------- 1 root root   76510702 Nov  5 11:02 PTT_IDTitle_content_string.txt\n","-rw------- 1 root root 1924254989 Nov  4 05:01 PTT_news.json\n","-rw------- 1 root root  681547474 Nov  5 04:57 PTT_news_v2.json\n","-rw------- 1 root root  257998246 Nov  5 08:34 PTT_news_v3.json\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KcTxZkJZNSTB","executionInfo":{"status":"ok","timestamp":1604577039814,"user_tz":-480,"elapsed":535,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}},"outputId":"5ca3b57e-64e9-447c-95c5-35ef459ac13d","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls -l /content/gdrive/My\\ Drive/DataMining/DataWithPTT/\n","file_dir = \"/content/gdrive/My Drive/DataMining/DataWithPTT/\""],"execution_count":14,"outputs":[{"output_type":"stream","text":["total 530993\n","-rw------- 1 root root 467225718 Nov  5 11:41 IDTitle_content_string.txt\n","-rw------- 1 root root  76510702 Nov  5 11:41 PTT_IDTitle_content_string.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Wl5G32emNz8P"},"source":["# 安裝ijson並將json file讀進來，轉成list"]},{"cell_type":"code","metadata":{"id":"qIQJ-WtdNznR","executionInfo":{"status":"ok","timestamp":1604454762030,"user_tz":-480,"elapsed":41561,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}},"outputId":"71d02d92-30a3-4bec-f51e-8b927465eff1","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install ijson\n","import ijson\n","# 使用ijson，以stream方式讀取json檔\n","f = open(file_dir + \"jsonJieba-tran.json\")\n","objects = ijson.items(f, 'item')\n","objects = list(objects)\n","\n","# 嘗試輸出\n","print(len(objects))\n","print(objects[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: ijson in /usr/local/lib/python3.6/dist-packages (3.1.2.post0)\n","876237\n","{'title': '海因裏希·希姆萊', 'content': '/海因裏希 nrt/· x/希姆萊 nr/海因裏希 nrt/· x/魯 nr/伊特 nrt/伯德 nr/· x/希姆萊 nr/（ x/德語 nz/： x/\\xa0 x/， x/） x/是 v/納粹德國 nr/的 uj/一名 m/重要 a/政治 n/頭目 n/， x/曾 d/為 p/內政部長 n/、 x/親衛隊 n/首領 n/， x/被 p/認為 v/對 p/歐洲 ns/600 m/萬 m/猶太人 nz/、 x/同性戀者 n/、 x/共產黨人 n/和 c/20 m/萬 m/至 p/50 m/萬羅姆 nr/人 n/的 uj/大屠殺 nz/以及 c/許多 m/武裝親衛隊 n/的 uj/戰爭 n/罪行 n/負有 v/主要 b/責任 n/。 x/二次大戰 nz/末期 f/企圖 n/與 zg/盟軍 n/單獨 d/談 v/和 c/失敗 v/， x/被 p/拘留 v/期間 f/服毒自殺 v/。 x/德國 ns/《 x/明鏡 n/》 x/週刊 n/中 f/對 p/希姆萊 nr/的 uj/評價 n/是 v/「 x/有史以來 l/最大 a/的 uj/劊子手 n/」 x/。 x/生平 n/. x/早年 t/. x/海因裏希 nrt/· x/希姆萊 nr/在 p/1900年 n/10月7日 n/出 v/生於 v/德意志帝國 n/巴伐利亞王國 n/的 uj/首都 d/慕尼黑 nr/希爾德 nrt/加街 ns/（ x/- x/- x/） x/2 m/號 m/3 m/樓 n/， x/是 v/家中 s/第 m/2 m/個兒 n/子 ng/。 x/父親 n/約瑟夫 nrt/· x/戈培 nr/哈特 nrt/· x/希姆萊 nr/（ x/Joseph eng/  x/Gebhard eng/  x/Himmler eng/） x/與 zg/巴伐利亞 ns/的 uj/威特 nrt/爾司 nr/巴赫 nr/皇室 n/關係 n/良好 a/， x/曾 d/擔任 v/皇室 n/顧問 nr/， x/海因裏希 nrt/親王 n/（ x/Heinrich eng/  x/Prinz eng/  x/von eng/  x/Bayern eng/） x/的 uj/家庭 n/老師 n/， x/之後 f/在 p/慕尼黑 nr/的 uj/一所 m/高等 b/中學 n/教書 n/。 x/母親 n/安娜 nr/· x/瑪利亞 ns/· x/海德 nrt/（ x/Anna eng/\\u3000 x/Maria eng/  x/Himmler eng/  x/Heyder eng/） x/則 d/是 v/一位 m/貿易 vn/商人 n/的 uj/女兒 n/， x/她 r/於 nr/1897年 n/與 p/戈培 nr/哈特 nrt/結婚 v/。 x/希姆萊 nr/在 p/家中 s/排行 v/第 m/2 m/， x/他 r/有個 r/哥哥 ns/戈培 nr/哈特 nrt/· x/盧德溫 nr/（ x/Gebhard eng/  x/Ludwig eng/， x/1898年 n/7月29日 n/出生 v/） x/和 c/弟弟 n/恩斯特 nrt/· x/赫爾曼 nr/（ x/Ernst eng/  x/Hermann eng/， x/1905年 n/12月23日 n/出生 v/） x/。 x/「 x/海因裏希 nrt/」 x/與 zg/「 x/魯 nr/伊特 nrt/伯德 nr/」 x/皆 d/是 v/由 p/皇室 n/授予 v/的 uj/名字 n/， x/尤其 d/是 v/海因裏希 nrt/， x/是 v/由 p/與 p/皇室 n/關係 n/良好 a/的 uj/戈培 nr/哈特 nrt/， x/請 v/曾 d/指導 n/過 ug/的 uj/學生 n/— x/海因裏希 nrt/親王 n/擔任 v/他 r/次子 n/的 uj/教父 n/並為 c/他 r/賜 v/名 q/。 x/當時 t/人們 n/認為 v/能 v/從 p/皇室 n/得到 v/賜 v/名 q/是 v/一種 m/社會 n/榮譽 nr/， x/而 c/海因裏希 nrt/則 d/就 d/以 p/自己 r/的 uj/名字 n/替 p/他 r/命名 n/， x/即 v/海因裏希 nrt/· x/希姆萊 nr/。 x/希姆萊 nr/的 uj/家庭 n/信仰 n/天主教 nz/， x/家風 n/保守 v/， x/並 c/對 p/小孩 n/嚴謹 a/教育 vn/禮儀 n/。 x/根據 p/戈培 nr/哈特 nrt/所 c/遺留 vn/的 uj/筆記 n/， x/希姆萊 nr/在 p/小學 n/時期 n/身體 n/非常 d/不好 d/， x/曾 d/缺席 n/160 m/次 t/， x/多虧 d/於 nr/家庭教師 n/的 uj/教導 n/補充 v/進度 d/， x/希姆萊 nr/後 nr/以 p/第 m/2 m/名 m/的 uj/成績 n/從 zg/小學 n/畢業 n/。 x/1910年 n/9月 n/， x/希姆萊 nr/至 p/慕尼黑 nr/威廉 nrt/文理中學 n/（ x/- x/- x/） x/就讀 v/； x/1913年 n/， x/由於 c/父親 n/將 d/赴 v/慕尼黑 nr/東北 ns/的 uj/蘭茨胡特 n/學校 n/作為 v/校長 n/而 c/自 r/該地 r/搬家 v/， x/並 c/就讀於 n/他 r/父親 n/所 c/管理 vn/的 uj/學校 n/。 x/希姆萊 nr/的 uj/歷史學 n/、 x/古典學 n/、 x/宗教學 n/為 p/最 a/優秀 a/的 uj/成績 n/， x/其他 r/主要 b/科目 n/成績 n/也 d/不錯 a/， x/唯獨 d/體育 vn/成績 n/欠佳 v/。 x/希姆萊 nr/後來 t/在 p/一次 m/大 a/戰後 t/的 uj/1919年 n/7月 n/才 d/從 p/學校 n/畢業 n/， x/畢業證書 n/上 f/的 uj/評語 nz/是 v/「 x/品行端正 n/， x/性格 n/勤勉 ad/而 c/守規矩 l/」 x/。 x/第一次世界大戰 nz/. x/第一次世界大戰 nz/的 uj/1915年 n/初 f/， x/希姆萊 nr/與 zg/哥哥 ns/戈培 nr/哈特 nrt/一起 m/加入 v/了 ul/青少年 nr/軍 zg/（ x/Jugendwehr eng/） x/， x/這 r/是 v/由 p/德國 ns/軍方 n/的 uj/校級 n/軍官 n/所 c/組織 v/的 uj/準軍事 n/組織 v/， x/主要 b/活動 vn/是 v/青少年 nr/進行 v/簡單 a/的 uj/軍事 n/遊行 v/和 c/運動 vn/。 x/之後 f/1915年 n/7月29日 n/， x/希姆萊 nr/的 uj/17 m/歲 m/哥哥 ns/戈培 nr/哈特 nrt/被 p/徵 g/招為 v/預備 v/軍 zg/（ x/Landsturm eng/） x/， x/並 c/在 p/1918年 n/4月 n/被 p/送往 v/西線 s/作戰 v/。 x/希姆萊 nr/自己 r/也 d/非常 d/想要 v/上 f/戰場 n/， x/卻 d/礙於 v/他 r/離 v/服役 n/年齡 n/還 d/差 a/一歲 m/， x/於是 c/求助於 v/父親 n/， x/父親 n/原 n/是 v/希望 v/他 r/完成學業 l/再說 c/， x/但 c/經不起 l/希姆萊 nr/的 uj/反覆 v/請求 v/， x/於是 c/求助 v/皇室 n/的 uj/朋友 n/， x/並 c/獲得 v/解決 v/。 x/希姆萊 nr/原想 n/加入 v/海軍 n/， x/但因為 c/帝國 n/海軍 n/不 d/收 v/近視 v/的 uj/人 n/， x/希姆萊 nr/只好 d/加入 v/陸軍 nr/， x/編入 v/巴伐利亞 ns/第 m/11 m/步兵團 n/「 x/森林 n/團 n/」 x/， x/希姆萊 nr/入伍 v/後 nr/被 p/派往 v/雷根斯堡 ns/接受 v/6 m/個 m/月 m/的 uj/步兵 n/訓練 vn/， x/在 p/1918年 n/6月15日 n/至 p/9月15日 n/期間 f/作為 v/預備士官 n/的 uj/訓練 vn/， x/其後 c/在 p/9月15日 n/至 p/10月1日 n/期間 f/於 nr/拜羅伊特 n/的 uj/第 m/17 m/機槍 n/班 n/訓練 vn/， x/完成 v/後 nr/被 p/編入 v/第 m/11 m/步兵團 n/補充 v/營 n/4 m/連 nr/。 x/但 c/在 p/希姆萊 nr/上 f/前線 n/前 f/， x/德國 ns/國內 s/發生 v/十一月革命 n/， x/威廉二世 n/被 p/推翻 v/， x/1918年 n/11月11日 n/德國 ns/宣佈 v/投降 v/， x/第一次世界大戰 nz/結束 v/， x/希姆萊 nr/則 zg/軍功 n/未建 v/而 c/退役 v/。 x/哥哥 ns/戈 nr/德哈特 ns/則 d/因為 c/在 p/西線 s/壕溝戰 n/中 f/表現出色 n/， x/先後 t/獲得 v/一級 m/與 zg/二級 b/的 uj/鐵十字勳章 n/； x/教父 n/海因裏希 nrt/親王 n/則 d/在 p/大戰 nz/中 f/戰死 n/， x/遺留 vn/了 ul/1000 m/帝國 n/馬克 nr/給 p/教子 n/希姆萊 nr/。 x/第一次世界大戰 nz/後 nr/. m/一次 m/大戰 nz/結束 v/後 f/， x/希姆萊 nr/仍 d/想待 v/在 p/軍隊 n/中 f/。 x/1919年 n/4月 n/， x/慕尼黑 nr/工人 n/在 p/共產黨 nt/領導 n/下 f/發動 vn/武裝 n/叛亂 v/， x/成立 v/了 ul/巴伐利亞蘇維埃共和國 n/， x/國內 s/右翼 n/份子 n/自行 r/組織 v/武裝 n/協助 v/正規軍 n/鎮壓 v/工人 n/叛亂 v/， x/最 d/出名 v/的 uj/團體 n/為 zg/「 x/自由軍團 n/」 x/， x/希姆萊 nr/加入 v/了 ul/它 r/的 uj/一支 m/小 a/志願團 n/， x/作為 v/後方 f/支援 v/。 x/之後 f/， x/因為 c/父親 n/歌德 nrt/哈特 nrt/認為 v/目前 t/德國經濟 n/混亂 a/且 zg/瀕臨破產 n/， x/最好 a/找 v/個 q/穩定 a/的 uj/工作 vn/， x/希姆萊 nr/就 d/在 p/1919年 n/春天 t/於 nr/慕尼黑 nr/北方 f/的 uj/因 c/戈爾 nr/施塔特 nr/一家 m/農場 n/找 v/了 ul/一份 m/工作 vn/； x/但因為 c/一場 m/嚴重 a/的 uj/斑疹傷寒 n/使 v/他 r/無法 n/再 d/從事 v/勞力 n/工作 vn/， x/在 p/長達 ns/一年 m/休養 v/時間 n/中 f/， x/希姆萊 nr/於 nr/1919年 n/10月18日 n/考進 v/了 ul/慕尼黑工業大學 n/， x/成為 v/一名 m/農技 n/學生 n/。 x/1919年 n/11月9日 n/， x/希姆萊 nr/加入 v/大學 n/學生 n/俱樂部 n/的 uj/學生 n/決鬥 v/團體 n/（ x/） x/， x/希望 v/在 p/決鬥 v/中 f/臉部 n/留下 v/傷痕 n/； x/當時 t/的 uj/德國 ns/男 n/大學生 n/認為 v/在 p/決鬥 v/時 ng/的 uj/臉部 n/傷害 a/是 v/一種 m/社會地位 n/象徵 v/。 x/由於 c/希姆萊 nr/的 uj/胃極 n/為 p/虛弱 a/而 c/不能 v/喝酒 v/， x/被 p/認定 v/為 zg/「 x/沒有 v/資格 n/參加 v/決鬥 v/」 x/， x/他 r/趕緊 d/去 v/醫院 n/申請 v/一張 m/腸胃 n/敏感 a/的 uj/證明 n/去 v/獲得 v/可以 c/決鬥 v/的 uj/承認 v/， x/然而 c/決鬥 v/對手 v/卻 d/仍 d/認為 v/希姆萊 nr/的 uj/身質 n/孱弱 a/。 x/1922年 n/6月22日 n/， x/希姆萊 nr/於 nr/畢業 n/不久前 t/的 uj/決鬥 v/中 f/被 p/打傷 v/， x/臉部 n/留下 v/傷痕 n/並 c/縫 v/了 ul/頭部 n/5 m/針 p/。 x/希姆萊 nr/大學 n/時期 n/身體 n/孱弱 a/與 p/對人 n/和善 nr/的 uj/表現 v/可以 c/從 p/他 r/的 uj/日記 n/中 f/看出 v/： x/1919年 n/多次 m/前去 t/盲人 n/家中 s/唸書 vn/給 p/他們 r/聽 v/、 x/1921年 n/在 p/貧窮 a/老婦 n/常 d/來往 v/的 uj/地方 n/偷偷 d/放置 v/食物 n/、 x/以 p/自己 r/和 c/家人 n/的 uj/身份 n/多次 m/探訪 v/生病 n/的 uj/友人 n/、 x/參與 v/維也納 ns/的 uj/慈善 nr/演出 v/等等 u/； x/此外 c/日記 n/顯示 v/， x/從 p/1921 m/起 v/希姆萊 nr/就 d/有 v/旅居 n/國外 s/的 uj/念頭 n/， x/這個 r/想法 v/直到 v/大學畢業 n/後 f/還是 c/有 v/， x/1924年 n/希姆萊 nr/還 d/曾 d/詢問 v/蘇聯 ns/大使館 n/能否 v/移居 v/到 v/烏克蘭 nr/。 x/1922年 n/8月1日 n/， x/希姆萊 nr/以 p/平均 a/評點 n/1.7 m/優異 a/的 uj/成績 n/畢業 n/， x/於 nr/8月5日 n/考取 v/農藝師 n/文憑 n/， x/並 c/在 p/奧伯 nr/施萊斯海姆 n/（ x/- x/- x/） x/找到 v/實驗室 n/助理 vn/的 uj/工作 vn/， x/但 c/1923年 n/8月 n/， x/希姆萊 nr/為 zg/專心 n/從事 v/政治 n/活動 vn/而 c/辭職 v/此 zg/工作 vn/， x/返回 v/慕尼黑 nr/。 x/希姆萊 nr/在 p/大學 n/時期 n/積極 ad/參加 v/右翼 n/政治 n/活動 vn/， x/1919年 n/12月 n/加入 v/了 ul/巴伐利亞 ns/人民黨 nt/（ x/Bayerische eng/  x/Volkspartei eng/， x/於 nr/1923年 n/退黨 n/） x/， x/1920年 n/5月 n/加入 v/慕尼黑 nr/市民 n/自衛軍 n/， x/從 p/威瑪 nrt/共和國 ns/的 uj/第 m/21 m/步槍團 n/聯隊 n/接收 v/了 ul/槍枝 n/與 zg/鋼盔 n/， x/武器 n/管理 vn/人 n/為 p/恩斯特 nrt/· x/羅姆 nr/。 x/希姆萊 nr/自 p/大學畢業 n/後 f/加入 v/了 ul/羅姆 nr/的 uj/準軍事 n/組織 v/— x/帝國 n/戰旗 n/（ x/Reichsbanner eng/） x/， x/1923年 n/加入 v/了 ul/國粹主義 n/的 uj/組織 v/— x/阿塔 nrt/曼納 nr/（ x/Artamanen eng/） x/。 x/對 p/希姆萊 nr/影響 vn/最大 a/的 uj/是 v/理查 v/· x/戴爾 nr/（ x/- x/- x/） x/的 uj/農本主義 n/著作 n/《 x/熱 zg/血與土 n/地 uv/》 x/， x/當 p/他 r/成為 v/親衛隊 n/全國 n/領袖 n/後 nr/便 d/邀請 v/了 ul/戴爾 nr/加入 v/親衛隊 n/； x/其他 r/還有 v/休斯頓 nrt/· x/斯圖爾特 nrt/· x/張伯倫 nr/的 uj/《 x/十九世紀 nz/的 uj/基礎 n/》 x/、 x/阿爾弗雷德 n/· x/羅森堡 nr/的 uj/《 x/二十世紀 nz/的 uj/神話 n/》 x/等 u/， x/這段 r/時間 n/裏 f/造就 v/他 r/以後 f/的 uj/國粹 n/思想 n/、 x/反猶主義 l/， x/也 d/是 v/納粹黨 nr/之後 f/的 uj/核心思想 n/。 x/納粹黨 nr/初期 t/的 uj/活動 vn/. x/1923年 n/8月 n/， x/希姆萊 nr/加入 v/了 ul/納粹黨 nr/， x/黨 n/證編號 n/14303 m/。 x/希姆萊 nr/作為 v/羅姆 nr/帝國 n/戰旗 n/的 uj/一名 m/成員 n/， x/參加 v/了 ul/他 r/所 u/領導 n/的 uj/巴伐利亞 ns/鎮壓 v/運動 vn/， x/但僅 c/是 v/一個 m/拿著 v/旗子 n/的 uj/跟班 v/， x/直到 v/今天 t/仍 d/有 v/保留 v/當時 t/的 uj/照片 n/， x/其後 c/也 d/以 p/旗手 n/的 uj/身份 n/參加 v/了 ul/失敗 v/啤酒館政變 n/， x/羅姆 nr/與 p/希特勒 nr/皆 d/被捕 v/下獄 v/， x/希姆萊 nr/則 zg/逃過 v/了 ul/牢獄之災 i/。 x/希姆萊 nr/於 nr/何時 c/與 zg/希特勒 nr/接觸 v/並 c/不 d/清楚 a/， x/一般 a/認為 v/是 v/在 p/希特勒 nr/出獄 v/， x/納粹黨 nr/重建 a/後 f/。 x/在 p/納粹黨 nr/被 p/禁止 v/活動 vn/的 uj/期間 f/， x/希姆萊 nr/加入 v/了 ul/埃裏希 nr/· x/魯登道 nr/夫 n/創立 v/的 uj/偽裝 n/政黨 n/— x/國家社會主義 n/自由黨 nt/（ x/Nationalsozialistische eng/  x/Freiheitspartei eng/， x/NSFP eng/） x/， x/在 p/納粹黨 nr/左派 m/的 uj/格里 nrt/哥 n/· x/斯特拉瑟 nrt/下 f/擔任 v/祕書 n/。 x/1924年 n/5月 n/和 c/12月 n/德國國會 nt/大選 v/， x/希姆萊 nr/為了 p/幫助 v/史 nr/特拉 nrt/塞 v/與其 c/黨員 n/競選 vn/， x/騎著 v/摩托車 nz/到處 d/宣傳 vn/民族 n/社會主義 n/、 x/反猶太 n/、 x/反共 d/思想 n/， x/因為 c/成效 a/卓越 nr/， x/史 nr/特拉 nrt/塞 v/獲得 v/了 ul/200 m/萬 m/選票 n/， x/32 m/個 m/國會 n/席位 n/， x/希姆萊 nr/也 d/因此 c/被 p/任命 n/為 zg/巴伐利亞 ns/- x/上 f/普法爾茨 nrt/省黨部 n/副 b/書記 n/。 x/1924年 n/12月 n/， x/希特勒 nr/自 p/蘭德斯 nr/堡 ng/（ x/Landsberg eng/） x/監獄 n/被 p/釋放 v/並 c/開始 v/重組 vn/納粹黨 nr/， x/而 c/1925年 n/2月 n/希姆萊 nr/與 p/史 nr/特拉 nrt/塞 v/回到 v/了 ul/納粹黨 nr/， x/希姆萊 nr/在 p/1926年 n/被 p/任命 n/為 zg/納粹黨 nr/全國 n/宣傳 vn/工作 vn/副 b/領導人 n/（ x/正 d/領導人 n/為 p/史 nr/特拉 nrt/塞 v/） x/， x/1927年 n/被 p/任命 n/為 zg/親衛隊 n/全國 n/副 b/領袖 n/。 x/希姆萊 nr/之前 f/在 p/1925年 n/之前 f/就 d/為 p/衝鋒隊 n/的 uj/成員 n/， x/但 c/在 p/1925年 n/8月8日 n/轉為 v/了 ul/黨衛隊 nt/， x/編號 n/168 m/。 x/1927年 n/， x/希姆萊 nr/成為 v/黨衛隊 nt/第三位 m/全國 n/領袖 n/艾爾 nr/哈德 nrt/· x/海登 nrt/（ x/Erhard eng/  x/Heiden eng/） x/的 uj/副手 n/， x/但因為 c/在 p/1929年 n/海登 nrt/被 p/人 n/告發 v/「 x/以 p/猶太人 nz/的 uj/廠商 n/來 zg/製作 vn/黨衛隊 nt/制服 v/」 x/而 c/被 p/開除 v/， x/身為 v/海登 nrt/左右手 m/的 uj/希姆萊 nr/也 d/因此 c/成為 v/了 ul/第 m/4 m/位 v/黨衛隊全國領袖 n/。 x/然而 c/， x/當時 t/的 uj/黨衛隊 nt/仍 zg/隸屬於 n/衝鋒隊 n/之下 f/， x/人數 n/也 d/不過 c/僅 d/280 m/人 n/。 x/同時 c/， x/希姆萊 nr/自己 r/也 d/是 v/衝鋒隊 n/大校 n/的 uj/身份 n/。 x/1928年 n/7月3日 n/， x/希姆萊 nr/與 p/一 m/名利 n/貝 zg/自由 a/州 n/（ x/Freistaat eng/  x/Lippe eng/） x/布隆堡 nr/（ x/Blomberg eng/） x/的 uj/地主 n/女兒 n/— x/瑪佳莉 nr/特 d/· x/玻登 nrt/（ x/Margarete eng/  x/Bode eng/） x/結婚 v/， x/瑪佳莉 nr/特是 d/一名 m/護士 n/， x/擁有 v/一家 m/小 a/診所 v/。 x/當時 t/德國 ns/物價飛漲 l/， x/希姆萊 nr/光是 n/靠 v/黨 n/的 uj/薪水 n/— x/月薪 n/200 m/馬克 nr/不足以 v/生活 vn/， x/之後 f/他們 r/賣掉 v/了 ul/瑪佳 nr/莉特 nrt/的 uj/診所 v/， x/在 p/慕尼黑 nr/郊區 s/蓋 v/了 ul/一間 m/小木屋 n/和 c/養雞場 n/， x/但 c/經營 vn/沒多久 l/就 d/破產 v/而 c/關閉 v/， x/他 r/和 c/瑪佳莉 nr/特 d/也 d/在 p/結婚 v/一年 m/後 nr/就 d/分居 v/。 x/親衛隊 n/全國 n/領袖 n/. x/海德里希 nrt/的 uj/出現 v/. x/希姆萊 nr/為 zg/成立 v/警察 v/組織 v/而 c/急著 v/擴展 v/黨衛隊 nt/人數 n/， x/1929年 n/12月 n/有 v/1000 m/名 n/， x/1930年 n/12月 n/有 v/2700 m/名 n/， x/1932年 n/4月 n/達到 v/了 ul/2 m/萬 m/5000 m/名 n/， x/1932年 n/12月 n/有 v/5 m/萬 m/2000 m/人 n/， x/順利 ad/增加 v/了 ul/隊員 n/人數 n/。 x/之所以 c/會 v/如此 c/是 v/因為 c/美國 ns/紐約市 ns/引起 v/的 uj/世界 n/經濟恐慌 i/， x/許多 m/失業者 n/對 p/政府 n/失望 v/， x/又 d/為 p/了 ul/有個 r/工作 vn/而 c/加入 v/了 ul/羅姆 nr/的 uj/衝鋒隊 n/， x/但因為 c/人數 n/過 ug/多 m/， x/素質 n/也 d/逐漸 d/下降 v/， x/甚至 d/連 nr/希特勒 nr/也 d/無法控制 nz/； x/這時 r/黨衛隊 nt/就 d/被 p/視為 v/制住 v/衝鋒隊 n/的 uj/一項 m/利器 n/， x/在 p/希特勒 nr/的 uj/默許 d/下 f/開始 v/公然 ad/擴展 v/， x/且 c/因為 c/黨衛隊 nt/是 v/以 p/對 p/希特勒 nr/個人 n/的 uj/效忠 n/為 zg/前提 n/而 c/建立 v/， x/希特勒 nr/也 d/比較 d/沒有 v/戒心 n/。 x/1930年 n/11月7日 n/， x/希特勒 nr/發佈 v/的 uj/命令 n/中 f/提到 v/「 x/黨衛隊 nt/最 d/首要 b/的 uj/任務 n/就是 d/在 p/黨內 s/執行 v/警察 v/職責 n/」 x/， x/希姆萊 nr/就為 v/了 ul/完成 v/此 zg/任務 n/而 c/開始 v/招攬 v/情報 n/機關 n/的 uj/人才 n/， x/這時 r/黨衛隊 nt/大校 n/， x/同時 c/也 d/是 v/希姆萊 nr/好友 n/的 uj/卡爾 nrt/· x/馮 nr/· x/愛貝 nr/斯坦因 nr/（ x/Karl eng/  x/von eng/  x/Eberstein eng/） x/男爵 n/向 p/他 r/推薦 v/了 ul/一名 m/前 f/海軍 n/武官 n/， x/然而 c/希姆萊 nr/因為 c/之前 f/去 v/帝國 n/海軍 n/應徵 v/被 p/拒絕 v/而 c/對 p/海軍 n/士兵 n/頗 d/為 p/反感 v/， x/多次 m/拖延 v/會面 n/， x/直到 v/男爵 n/一再 d/堅持 v/希姆萊 nr/才 d/勉強 v/接見 v/他 r/。 x/1931年 n/6月14日 n/， x/希姆萊 nr/接見 v/了 ul/前來 t/面試 v/的 uj/萊因哈特 nrt/· x/海德里希 nrt/， x/一 m/開始 v/他 r/想要 v/刁難 a/一下 m/這位 rz/前 f/海軍 n/武官 n/， x/他 r/對 p/海德里希 nrt/出 v/了 ul/一道 m/難題 n/， x/他 r/說 v/： x/「 x/我 r/想要 v/在 p/黨內 s/設置 vn/一個 m/專門機構 n/， x/主管 n/情報工作 n/， x/這 zg/需要 v/一位 m/專家 n/， x/如果 c/你 r/認為 v/你 r/能 v/勝任 n/這 r/一 m/工作 vn/， x/請 v/在 p/這 r/張 q/紙 n/上 f/寫出 v/你 r/的 uj/構想 v/， x/時間 n/只有 c/20 m/分鐘 q/。 x/」 x/海德里希 nrt/之前 f/只是 c/一位 m/通訊 nz/官 n/， x/對 p/情報工作 n/雖 zg/不 d/陌生 n/但 c/所知 v/有限 a/， x/但 c/他 r/還是 c/寫出 v/一張 m/黨內 s/情報工作 n/機關 n/的 uj/運作 vn/藍圖 nr/， x/希姆萊 nr/認為 v/海德里希 nrt/是 v/「 x/一部 m/活 n/的 uj/記錄器 n/」 x/， x/是 v/一個 m/「 x/天生 n/的 uj/情報 n/人才 n/」 x/， x/當場 s/讓 v/他 r/入黨 n/， x/負責 v/黨內 s/的 uj/情報工作 n/， x/黨 n/編號 n/544916 m/號 m/。 x/之後 f/設置 vn/了 ul/IC eng/科 n/， x/隔 v/年 m/成 v/了 ul/帝國 n/保安 nz/部 n/， x/為 zg/納粹黨 nr/第一個 m/情報 n/機關 n/， x/海德里希 nrt/為 zg/實際 n/的 uj/執行長 n/， x/希姆萊 nr/則 d/是 v/擔任 v/名義 n/上 f/的 uj/領袖 n/。 x/1931年 n/4月 n/初 f/， x/柏林 nr/的 uj/衝鋒隊 n/軍官 n/瓦爾特 nrt/· x/史坦斯 nr/（ x/Walther eng/  x/Stennes eng/） x/對 p/慕尼黑 nr/的 uj/納粹黨 nr/本部 r/起頭 v/叛亂 v/， x/柏林 nr/的 uj/黨衛隊 nt/領導人 n/庫爾特 nrt/· x/達 v/呂格 nr/（ x/Kurt eng/  x/Daluege eng/） x/領軍 n/鎮壓 v/。 x/此 zg/事件 n/後 f/希特勒 nr/對 p/黨衛隊 nt/有 v/了 ul/不同 a/的 uj/看法 v/， x/給予 v/了 ul/獨立 v/於 nr/衝鋒隊 n/的 uj/自由性 d/和 c/作為 v/黨內 s/警察 v/的 uj/職責 n/， x/並 c/任命 n/希姆萊 nr/為 zg/納粹黨 nr/黨部 n/棕屋 n/（ x/Braunes eng/  x/Haus eng/） x/的 uj/總 n/警備 n/， x/負責 v/「 x/防止 v/共產 n/份子 n/和 c/任何 r/干擾 v/黨 n/的 uj/活動 vn/」 x/任務 n/。 x/之後 f/希姆萊 nr/想要 v/使 v/黨衛隊 nt/變成 v/一個 m/除了 p/專門 n/護衛 nz/希特勒 nr/外 f/， x/還要 c/是 v/種族政策 n/的 uj/模範 n/與 zg/核心 n/， x/新 a/進 v/隊員 n/必須 d/為 zg/「 x/優良 z/血統 n/」 x/的 uj/亞利安人 nrt/種 m/， x/以 p/血統證明 n/嚴格 ad/限制 v/入隊 n/的 uj/素質 n/。 x/希姆萊 nr/設立 v/了 ul/種族 n/及 c/再 d/安置 v/本部 r/（ x/Rasse eng/  x/und eng/  x/Siedlungshauptamt eng/， x/即是 c/RuSHA eng/） x/， x/由 p/理查 v/· x/戴爾 nr/領導 n/調查 vn/所有 b/入隊 n/隊員 n/的 uj/身家 n/背景 n/， x/隊員 n/們 k/不能不 d/經過 p/RuSHA eng/的 uj/調查 vn/和 c/許可 nr/擅自 d/結婚 v/， x/確認 v/新娘 n/是否 v/有 v/遺傳 vn/問題 n/， x/是否 v/為 zg/同級 b/別的 r/人種 n/才 d/會 v/獲得 v/許可 nr/； x/結婚 v/後 f/， x/RuSHA eng/要求 v/隊員 n/有 v/義務 n/要 v/至少 d/養 v/一個 m/小孩 n/， x/沒有 v/子嗣 nr/者 k/會 v/領 n/不到 v/一部分 m/的 uj/薪水 n/， x/希姆萊 nr/此 zg/規定 n/是 v/為 p/大量 n/培養 v/「 x/純 a/血 n/的 uj/日耳曼人 n/後代 t/」 x/。 x/1932年 n/7月7日 n/， x/希姆萊 nr/為了 p/黨衛隊 nt/的 uj/獨立性 n/和 c/區別 n/衝鋒隊 n/， x/要求 v/更改 v/黨衛隊 nt/的 uj/制服 v/， x/就 d/成 v/了 ul/有名 a/的 uj/黨衛隊 nt/「 x/黑衫 n/服 v/」 x/， x/一般 a/認為 v/黑衫 n/服是 n/仿自 d/義大利 nz/法西斯黨 ns/的 uj/黑衫軍 n/服飾 n/， x/但 c/實際上 d/是 v/源於 v/普魯士王國 n/的 uj/禁軍 n/軍服 n/（ x/黨衛隊 nt/軍帽 n/上 f/的 uj/骷髏頭 vn/也 d/是 v/普魯士 nr/禁軍 n/標誌 n/） x/。 x/納粹黨掌權 n/. x/1933年 n/1月30日 n/， x/興登堡 n/總統 n/任命 n/希特勒 nr/為 zg/德國總理 nr/， x/多數 m/納粹黨 nr/黨內 s/幹部 n/都 d/紛紛 d/獲取 v/中央 n/與 zg/各州 r/的 uj/重要 a/官職 n/， x/而 c/希姆萊 nr/則 d/被 p/冷落 v/於一 n/旁 f/。 x/3月9日 n/， x/在 p/巴伐利亞 ns/總理 n/海因裏希 nrt/· x/海德 nrt/（ x/Heinrich eng/  x/Held eng/） x/被 p/衝鋒隊 n/和 c/黨衛隊 nt/罷除 v/職務 n/後 f/， x/希姆萊 nr/才 d/擔任 v/了 ul/慕尼黑 nr/警察局長 n/一 m/職 n/， x/雖然 c/對此 d/安排 v/不滿 a/， x/希姆萊 nr/還是 c/盡力去做 i/該 r/職務 n/， x/並 c/任命 n/海德里希 nrt/為 p/第 m/6 m/部 n/（ x/政治部 n/） x/部長 n/， x/不斷 d/消滅 v/黨 n/的 uj/政治 n/敵人 n/。 x/巴伐利亞 ns/邦 ng/司法部長 n/漢斯 nz/· x/法郎 n/克 m/認為 v/應該 v/將 zg/所有 b/政治 n/敵人 n/全部 n/關 v/到 v/集中營 b/， x/希姆萊 nr/因此 c/於 nr/3月20日 n/設立 v/了 ul/達豪集中營 n/並 c/開始 v/管理 vn/， x/為 zg/納粹德國 nr/的 uj/第一個 m/集中營 b/。 x/4月1日 n/， x/希姆萊 nr/成為 v/巴伐利亞州 ns/警察局 n/局長 n/。 x/1933年 n/9月 n/， x/因為 c/希特勒 nr/對於 i/自身 r/安全 an/問題 n/越來越 d/擔心 v/， x/而 c/國防軍 nt/又 d/不對 d/他 r/效忠 n/， x/他 r/下令 v/給 p/希姆萊 nr/組織 v/一個集 m/黨衛隊 nt/精銳 a/團體 n/， x/保護 v/希特勒 nr/的 uj/私人 n/衛隊 n/— x/阿道夫 nr/· x/希特勒 nr/警衛 n/旗隊 n/（ x/Leibstandarte eng/  x/Adolf eng/  x/Hitler eng/， x/即是 c/LAH eng/） x/， x/由 p/約瑟夫 nrt/· x/迪特里希 n/指揮 v/， x/該 r/組織 v/即 v/是 v/後來 t/第一 m/阿道夫 nr/· x/希特勒 nr/警衛 n/旗隊 n/裝甲師 n/的 uj/前身 r/。 x/但 c/迪特里希 n/將 d/此 r/隊伍 n/認為 v/僅 d/負責 v/保護 v/希特勒 nr/， x/而 c/忽略 d/希姆萊 nr/的 uj/指揮 v/地位 n/， x/兩人 n/常有 b/相關 v/的 uj/指揮權 n/爭論 v/。 x/統一 vn/警察 v/. x/希特勒內閣 n/內政部長 n/的 uj/威廉 nrt/· x/弗利 nrt/克 m/提出 v/國家 n/一體化 l/（ x/Gleichschaltung eng/） x/的 uj/政策 n/， x/將 zg/各州 r/自治權 n/由 p/中央 n/所 c/管理 vn/， x/1934年 n/1月 n/， x/除了 p/普魯士 nr/自由 a/州 n/與 p/紹姆堡 ns/- x/利佩州 nr/（ x/Schaumburg eng/- x/Lippe eng/） x/外 f/， x/全 a/德國 ns/的 uj/警察 v/由 p/希姆萊 nr/所 c/管轄 v/。 x/普魯士 nr/自由 a/州 n/佔 v/當時 t/德國 ns/國土面積 n/一半 m/， x/戈林 nr/是 v/當地 s/警察 v/首腦 n/， x/擔心 v/希姆萊 nr/與 zg/海德里希 nrt/的 uj/警察 v/權力 n/延伸 v/到 v/普魯士 nr/， x/曾 d/以 p/蓋世太保 nr/暴力 n/威脅 vn/海德里希 nrt/滾出 v/柏林 nr/。 x/希姆萊 nr/於是 c/向 p/興登堡 n/總統 n/告發 v/戈林 nr/部下 n/魯道夫 nr/· x/迪爾斯 n/（ x/Rudolf eng/  x/Diels eng/） x/的 uj/各種 r/醜聞 n/( x/蓋世太保 nr/創始人 n/) x/， x/使 v/戈林 nr/倍感 n/壓力 n/。 x/另外 c/一方面 mq/， x/羅姆 nr/的 uj/衝鋒隊 n/也 d/在 p/急速 d/擴展 v/勢力 n/（ x/1933年 n/時 zg/高達 nr/300 m/萬 m/人 n/） x/， x/威脅 vn/到 v/了 ul/戈林 nr/。 x/在 p/希姆萊 nr/與 p/羅姆 nr/兩個 m/政治 n/敵人 n/之間 f/， x/戈林 nr/選擇 v/聯合 v/希姆萊 nr/打擊 v/羅姆 nr/的 uj/衝鋒隊 n/， x/並 c/與 p/他 r/和解 v/。 x/1934年 n/4月20日 n/， x/戈林 nr/在 p/迪爾斯 n/的 uj/蓋世太保 nr/局長 n/上 f/再 d/加上 v/一個 m/職位 n/「 x/蓋世太保 nr/監查 vn/官 n/與 p/代理 n/指揮官 n/」 x/（ x/Inspekteur eng/  x/und eng/  x/stellvertretender eng/  x/Chef eng/  x/der eng/  x/Geheimen eng/  x/Staatspolizeiamts eng/） x/， x/並 c/讓 v/希姆萊 nr/擔任 v/此位 r/， x/希姆萊 nr/就 d/立刻 d/除去 t/依賴 v/戈林 nr/的 uj/迪爾斯 n/， x/將 d/他 r/的 uj/職位 n/「 x/蓋世太保 nr/局長 n/」 x/給 p/了 ul/海德里希 nrt/。 x/戈林 nr/到 v/1935年 n/11月20日 n/之前 f/雖 c/仍 d/是 v/蓋世太保 nr/的 uj/最高 a/領袖 n/， x/但 c/實權 n/已經 d/轉交 v/給 p/了 ul/希姆萊 nr/。 x/其後 c/所有 b/警察 v/權力 n/漸 d/落入 v/希姆萊 nr/手中 s/， x/希姆萊 nr/最後 f/成 v/了 ul/全 m/德國 ns/的 uj/警察 v/領袖 n/。 x/長刀之夜 n/. x/在 p/希姆萊 nr/取得 v/全國 n/警察 v/控制權 n/的 uj/同時 c/， x/羅姆 nr/也 d/在 p/對 p/國防軍 nt/要求 v/將 zg/容克 nr/軍官 n/拔除 v/， x/以 p/衝鋒隊 n/取代 v/正規軍 n/並 c/取得 v/政治 n/地位 n/。 x/隨著 v/兩方 m/的 uj/激烈 a/衝突 vn/， x/希特勒 nr/被 p/請來 v/解決 v/此 zg/問題 n/， x/但 c/他 r/因為 c/與 p/羅姆曾 nr/是 v/老戰友 n/而 c/猶豫不決 i/， x/希姆萊 nr/也 d/曾 d/是 v/羅姆 nr/的 uj/跟班 v/， x/也 d/同樣 d/感到 v/焦慮 a/， x/但 c/海德里希 nrt/告訴 v/希姆萊 nr/「 x/為了 p/黨衛隊 nt/的 uj/未來 t/也 d/應該 v/要 v/加以 v/肅清 v/。 x/」 x/希姆萊 nr/也 d/因此 c/決定 v/要 v/下手 v/消滅 v/羅姆 nr/。 x/國防軍 nt/領導人 n/維爾納 nr/· x/馮 nr/· x/勃洛姆堡 nrt/與 p/羅姆 nr/談判 vn/， x/說 zg/衝鋒隊 n/可以 c/負責 v/士兵 n/的 uj/入伍 v/訓練 vn/和 c/退伍 v/事宜 n/， x/但 c/國防軍 nt/是 v/全 n/德國 ns/唯一 b/的 uj/武裝部隊 n/， x/但 c/羅姆 nr/一意孤行 i/， x/不 d/理會 n/希特勒 nr/的 uj/調解 v/， x/說道 v/： x/「 x/我們 r/纔 zg/不會 v/按 p/協議 n/辦事 n/呢 y/。 x/希特勒 nr/言而無信 i/， x/少 a/說 v/也 d/得 ud/去 v/休假 v/； x/要是 c/希特勒 nr/不 d/願意 v/一起 m/幹 v/， x/我們 r/就 d/撇開 v/他 r/來 v/幹 v/」 x/， x/但 c/希特勒 nr/仍 d/因為 c/多年 m/的 uj/交情 n/而 c/猶豫 a/再三 d/。 x/希姆萊 nr/與 zg/海德里希 nrt/在 p/戈林 nr/的 uj/幫助 v/下 f/擬定 v/了 ul/一份 m/需要 v/剷除 v/的 uj/衝鋒隊 n/領導人 n/名單 n/， x/並 c/捏造 v/了 ul/羅姆 nr/打算 v/反叛 v/的 uj/證據 n/， x/要求 v/希特勒 nr/處決 v/羅姆 nr/， x/希特勒 nr/在 p/周圍 f/人士 n/的 uj/影響 vn/下 f/同意 d/了 ul/。 x/1934年 n/6月30日 n/， x/當晚 t/即 v/是 v/有名 a/的 uj/「 x/長刀之夜 n/」 x/， x/黨衛隊 nt/處決 v/了 ul/羅姆 nr/、 x/衝鋒隊 n/領袖 n/與 zg/許多 m/希特勒 nr/潛在 t/的 uj/政治 n/敵人 n/， x/包括 v/希姆萊 nr/的 uj/前任 n/上司 n/史 nr/特拉 nrt/塞 v/。 x/1934年 n/7月20日 n/， x/希特勒 nr/說道 v/： x/「 x/鑑於 p/黨衛隊 nt/在 p/1934年 n/6月30日 n/事件 n/中 f/所 c/做出 v/的 uj/巨大 a/功績 n/， x/我 r/將 d/其 r/升格 n/為 zg/納粹黨 nr/內 f/的 uj/一個 m/獨立 v/組織 v/。 x/」 x/此 zg/事件 n/後 f/， x/勃洛姆堡 nrt/對於 i/黨衛隊 nt/的 uj/功績 n/給予 v/正面 ad/評價 n/， x/並 c/承認 v/其 r/3 m/個團 n/的 uj/黨衛隊 nt/特務 n/部隊 n/（ x/SS eng/- x/Verf eng/ü x/gungstruppe eng/， x/即是 c/SS eng/- x/VT eng/） x/擁有 v/武裝 n/兵力 n/特權 n/。 x/希姆萊 nr/則 zg/希望 v/持續 vd/擴張 v/目前 t/的 uj/軍力 n/， x/這 r/也 d/成為 v/之後 f/的 uj/武裝親衛隊 n/前身 r/， x/黨衛隊 nt/在 p/黨內 s/的 uj/勢力 n/也 d/變大 v/了 ul/。 x/黨衛隊 nt/產業 n/. x/羅姆死 nr/後 nr/， x/國內 s/所有 b/集中營 b/就 d/成 v/了 ul/希姆萊 nr/管轄 v/， x/他 r/將 d/達豪集中營 n/管理 vn/人 n/西奧多 nrt/· x/艾克 nr/任命 n/為 p/全 n/集中營 b/和 c/守衛 v/部隊 n/— x/骷髏 n/部隊 n/（ x/SS eng/- x/Totenkopfverb eng/ä x/nde eng/， x/即是 c/SS eng/- x/TV eng/） x/總監 n/。 x/另外 c/希姆萊 nr/還 d/任命 n/前 f/海軍 n/會計 v/的 uj/歐 n/斯瓦德 nr/· x/波赫 nr/（ x/Oswald eng/  x/Ludwig eng/  x/Pohl eng/） x/為 zg/黨衛隊 nt/本部 r/經濟部長 n/， x/負責 v/經營 vn/德國 ns/土石 n/製造 v/有限公司 n/（ x/DEST eng/） x/、 x/德國 ns/裝備 n/製造 v/有限公司 n/（ x/DAW eng/） x/和 c/阿 ns/波利納 n/里斯 nrt/礦泉水 n/（ x/Apollinaris eng/） x/等 u/企業 n/， x/並 c/從 p/集中營 b/的 uj/囚犯 n/取得 v/大量 n/勞動力 n/， x/連艾克 nr/的 uj/管理 vn/地位 n/也 d/在 p/其 r/之下 f/； x/希姆萊 nr/也 d/關心 n/著 n/黨衛隊 nt/的 uj/陶瓷 n/生意 v/— x/阿拉 nrt/契 zg/瓷器 n/， x/儘管 c/經常 d/赤字 n/且 zg/會計 v/勸告 v/休業 v/， x/希姆萊 nr/仍 zg/堅持 v/持續經營 n/； x/其他 r/還有 v/以 p/威脅 vn/、 x/類似 v/恐嚇 v/的 uj/方式 n/從 p/各 r/大 d/公司 n/收取 v/「 x/保護費 n/」 x/， x/包括 v/： x/西門子 nr/、 x/中 f/德 ns/鋼鐵廠 n/、 x/法本公司 n/等 u/， x/1932年 n/希姆萊 nr/納款 n/了 ul/1 m/萬 m/7 m/千 m/馬克 nr/， x/1933年 n/35 m/萬馬克 nr/， x/1934年 n/有 v/58 m/萬 m/1 m/千 m/馬克 nr/。 x/整合 v/警察機關 n/. x/1936年 n/6月17日 n/， x/希姆萊 nr/正式 ad/成為 v/全 n/德國 ns/警察 v/總長 n/， x/他 r/趁 v/此時 c/改編 v/警察機關 n/的 uj/制度 n/， x/在 p/每個 r/一般 a/警察 v/中 f/再訂 v/了 ul/一個 m/秩序 n/警察 v/（ x/Ordnungspolizei eng/） x/， x/由 p/黨衛隊 nt/上將 n/庫爾特 nrt/· x/達 v/呂格 nr/管理 vn/。 x/另一方面 c/將 zg/政治 n/警察 v/的 uj/蓋世太保 nr/與 zg/刑事警察 n/合併 v/為 zg/保安警察 n/， x/由 p/海德里希 nrt/管理 vn/； x/接著 v/又 d/設立 v/黨衛隊 nt/與 zg/警察 v/高級 b/領導人 n/（ x/H x/ö x/here eng/  x/SS eng/  x/und eng/  x/Polizeif eng/ü x/hrer eng/、 x/即是 c/HSSPF eng/） x/的 uj/職位 n/， x/配置 v/各 r/人員 n/在 p/德國 ns/各 r/警察機關 n/。 x/1939年 n/9月27日 n/， x/希姆萊 nr/將 zg/保安 nz/服務處 n/與 zg/保安警察 n/合併 v/， x/成為 v/黨衛隊國家安全部 n/在 p/黨衛隊 nt/滲透 v/警察機關 n/的 uj/同時 c/， x/希姆萊 nr/也 d/讓 v/其 r/幹預 v/了 ul/各 r/層面 n/的 uj/政治 n/活動 vn/， x/包括 v/清算 v/國防部長 n/勃洛姆堡 nrt/與 p/一級 m/上將 n/魏勒 nr/· x/馮 nr/· x/弗理奇 nr/， x/以 p/製造 v/醜聞 n/的 uj/方式 n/逼 v/他們 r/辭去 v/職位 n/。 x/在 p/海外 s/也 d/對 p/蘇聯 ns/產生 n/一定 d/影響力 n/的 uj/處死 v/米哈伊爾 nrt/· x/尼古拉 nrt/耶維奇 nr/· x/圖哈 nrt/切夫斯基 nrt/並 c/促成 v/大清洗 n/， x/使 v/成千上萬 i/的 uj/蘇聯 ns/領導人 n/死亡 v/。 x/在 p/德奧合併 n/事件 n/中 f/， x/黨衛隊 nt/成員 n/也 d/殺害 v/了 ul/反對者 n/— x/奧地利總理 n/恩格爾 nr/伯特 nr/· x/陶爾斐 nr/斯 nr/。 x/第二次世界大戰 nz/. x/1939年 n/8月 n/， x/希特勒 nr/命令 n/希姆萊 nr/製造 v/一個 m/可以 c/進攻 v/波蘭 ns/的 uj/藉口 nr/， x/海德里希 nrt/將 d/此 r/計畫 n/稱為 v/「 x/希姆萊 nr/行動 vn/」 x/。 x/由 c/SD eng/特務 n/人員 n/阿爾弗雷德 n/· x/納 zg/傑克 nrt/（ x/Alfred eng/  x/Helmut eng/  x/Naujocks eng/） x/穿著 n/波蘭 ns/軍服 n/在 p/格利維採 n/電視臺 n/上 f/發表 v/反德 n/演說 v/， x/並 c/開槍 v/射殺 vn/準備 v/好 a/的 uj/德國 ns/平民 n/（ x/從 zg/集中營 b/拉來 v/的 uj/囚犯 n/） x/， x/希特勒 nr/宣稱 v/「 x/波蘭 ns/與 zg/德國 ns/已 d/進入 v/戰爭狀態 n/」 x/， x/並 c/在 p/國會 n/上 f/對 p/波蘭 ns/宣戰 v/。 x/但是 c/大戰 nz/初期 t/， x/仍 d/有 v/許多 m/事件 n/折損 v/了 ul/希特勒 nr/對 p/希姆萊 nr/的 uj/安全 an/信任 n/： x/1939年 n/11月8日 n/， x/希特勒 nr/在 p/貝格勃勞凱勒啤酒館 n/發表 v/啤酒館政變 n/16 m/週年紀念 n/演說 v/， x/他 r/提早 v/離開 v/後 f/險些 d/被 p/藏 j/在 p/柱子 n/內 f/的 uj/炸彈 n/炸死 v/， x/該 r/事件 n/造成 v/7 m/人 n/死亡 v/， x/63 m/人 n/受傷 v/。 x/放置 v/炸彈 n/的 uj/約翰 nrt/· x/艾爾塞 nr/（ x/Johann eng/  x/Georg eng/  x/Elser eng/） x/企圖 n/趁夜 d/非法 b/入境 n/瑞士 ns/逃走 v/， x/希特勒 nr/認為 v/艾爾塞 nr/是 v/英國 ns/派來 v/的 uj/刺客 n/， x/希姆萊 nr/對 p/他 r/一番 m/嚴刑拷打 vn/後 f/， x/艾爾塞 nr/供出 v/他 r/是 v/單獨 d/犯案 n/， x/與 zg/英國 ns/無關 v/， x/希姆萊 nr/因為 c/讓 v/英國 ns/替 p/自己 r/承擔 v/安全 an/問題 n/上 f/的 uj/疏失 v/失敗 v/， x/招來 v/希特勒 nr/一頓 m/責罵 v/。 x/1941年 n/1月20日 n/， x/原本 n/支持 v/羅馬尼亞王國 n/獨裁者 n/揚 vg/· x/安東 ns/內斯庫 nr/的 uj/鐵衛團 n/叛亂 v/， x/希特勒 nr/與 zg/外交部長 n/約阿希姆 nrt/· x/馮 nr/· x/裏賓 ns/特洛甫 nr/支持 v/安東 ns/內斯庫 nr/， x/並 c/派軍 n/平亂 a/； x/但 c/希姆萊 nr/、 x/海德里希 nrt/與 p/SD eng/人員 n/卻 d/因為 c/與 p/鐵衛團 n/交情 n/不錯 a/而 c/救出 v/幾個 m/霍里亞 nr/· x/希瑪 nrt/（ x/Horia eng/  x/Sima eng/） x/的 uj/手下 s/鐵衛 nr/團幹部 n/， x/此事 r/導致 v/希特勒 nr/非常 d/不滿 a/， x/當場 s/將 zg/幾個 m/SD eng/人員 n/處分 n/。 x/裏賓 ns/特洛甫 nr/認為 v/這 r/是 v/止住 v/黨衛隊 nt/勢力 n/的 uj/好 a/機會 n/， x/因為 c/隨著 v/戰爭 n/的 uj/進行 v/， x/外交部 nt/的 uj/功能 n/日趨 d/減少 v/， x/裏賓 ns/特洛甫 nr/就 d/多次 m/與 p/SA eng/衝突 vn/， x/想要 v/鞏固 v/自己 r/的 uj/權力 n/。 x/1942年 n/6月4日 n/， x/身兼 v/帝國 n/保安 nz/部 n/部長 n/與 zg/波希米亞和摩拉維亞保護國 n/副 b/總督 n/兩 m/職 ng/的 uj/海德里希 nrt/在 p/一次 m/出訪 v/時 ng/被 p/英國 ns/派來 v/的 uj/捷克 ns/暗殺 v/部隊 n/暗殺 v/， x/希姆萊 nr/就 d/暫時 d/擔任 v/帝國 n/保安 nz/部 n/表面 n/上 f/的 uj/部長 n/， x/實際 n/職務 n/由 p/布魯諾 nr/· x/史崔克 nr/貝齊 nr/（ x/Bruno eng/  x/Streckenbach eng/） x/負責 v/。 x/之後 f/才 d/把 p/此 r/職務 n/正式 ad/交給 v/黨衛隊 nt/上將 n/恩斯特 nrt/· x/卡爾 nrt/滕 nr/布倫納 nr/。 x/1943年 n/8月24日 n/， x/希姆萊 nr/取代 v/威廉 nrt/· x/弗利 nrt/克成 nr/為 p/納粹德國 nr/的 uj/內政部長 n/， x/而 c/弗利 nrt/克則成 nr/為 p/波希米亞和摩拉維亞保護國 n/總督 n/。 x/猶太人 nz/大屠殺 nz/. x/從 zg/開戰 v/前到 v/戰爭初期 n/， x/希姆萊 nr/僅 zg/命令 n/黨衛隊 nt/將 zg/猶太人 nz/驅逐出境 n/， x/但 c/到 v/了 ul/1938年 n/， x/調職 vn/到 v/奧地利 ns/「 x/猶太人 nz/移民局 n/」 x/局長 n/與 p/身為 v/SD eng/人員 n/的 uj/阿道夫 nr/· x/艾 nr/希曼 nrt/， x/他 r/在 p/1939年 n/1月 n/提出 v/將 zg/猶太人 nz/集中 v/到 v/國內 s/， x/再 d/分批 m/送到 v/管理 vn/地點 n/， x/甚至 d/曾 d/有 v/以 p/「 x/政治 n/解決 v/」 x/的 uj/考量 n/。 x/1939年 n/10月7日 n/， x/希特勒 nr/任命 n/希姆萊 nr/為 zg/「 x/德國 ns/民族 n/強化 v/委員會 n/帝國 n/成員 n/」 x/（ x/Reichskommisar eng/  x/f x/ü x/rdie eng/  x/Festigung eng/  x/des eng/  x/deutschen eng/  x/Volkstums eng/） x/， x/命令 n/他 r/完成 v/所有 b/德國 ns/佔領區 nr/內 f/的 uj/「 x/日爾曼 ns/化 n/」 x/工作 vn/， x/希姆萊 nr/即 v/在 p/黨衛隊 nt/本部 r/設立 v/德國 ns/民族 n/強化 v/帝國 n/委員會 n/本部 r/（ x/RKFDV eng/） x/， x/由 p/烏爾裏 nrt/希 v/· x/格里弗 nr/特 d/（ x/Ulrich eng/  x/Heinrich eng/  x/Emil eng/  x/Richard eng/  x/Greifelt eng/） x/指揮 v/， x/以 p/亞利安人 nrt/統治 v/地位 n/為 zg/核心 n/概念 n/， x/開始 v/將 zg/往東方 n/殖民 n/擴張 v/、 x/將 zg/歐洲 ns/、 x/東方 s/的 uj/猶太人 nz/強制 v/拘留 v/， x/送到 v/集中營 b/管理 vn/。 x/1939年 n/侵略 v/波蘭 ns/後 f/， x/帝國 n/保安 nz/部 q/在 p/佔領區 nr/與 zg/蘇聯 ns/合作 vn/， x/將 zg/反抗 v/政策 n/的 uj/波蘭人 nz/全數 n/槍殺 v/。 x/這時 r/仍 d/還 d/沒有 v/使 v/猶太人 nz/滅絕 v/的 uj/想法 v/， x/希姆萊 nr/在 p/1940年 n/5月 n/的 uj/日記 n/中 f/寫道 v/： x/「 x/以 p/布爾什維克 nrt/的 uj/方法 n/來 zg/滅絕 v/猶太人 nz/並非 c/日爾曼 ns/所能 b/的 uj/， x/是 v/不 d/可能 v/的 uj/」 x/。 x/希姆萊 nr/對 p/反對 d/屠殺 vn/的 uj/將軍 n/說道 v/： x/是 v/希特勒 nr/對 p/他 r/下達 v/屠殺 vn/的 uj/命令 n/， x/此時 c/已有 v/25 m/萬 m/波蘭 ns/籍 ng/猶太人 nz/被 p/黨衛隊 nt/殺害 v/， x/而 c/希特勒 nr/一般 a/被 p/認為 v/是 v/在 p/1941年 n/夏 nr/決定 v/要 v/將 d/猶太人 nz/「 x/滅絕 v/」 x/， x/而 c/實際 n/工作 vn/就 d/交由 n/希姆萊 nr/的 uj/黨衛隊 nt/負責 v/。 x/1941年 n/夏 nr/， x/德軍 nr/發動 vn/巴巴羅薩 n/作戰 v/入侵 v/蘇聯 ns/， x/開啟 v/蘇德戰爭 n/， x/黨衛隊 nt/的 uj/別動隊 n/跟隨 v/德意志國防軍 n/進攻 v/， x/將 d/所到 v/之 u/地 uv/的 uj/猶太人 nz/全部 n/抓獲 v/， x/大量 n/屠殺 vn/。 x/1941年 n/8月 n/， x/希姆萊 nr/召見 v/波蘭 ns/的 uj/奧斯威辛集中營 ns/營長 n/魯道夫 nr/· x/霍斯 nr/到 v/柏林 nr/， x/命令 n/他 r/準備 v/滅絕 v/歐洲 ns/猶太人 nz/， x/並 c/改建 v/集中營 b/為 zg/毒氣室 n/， x/從此 c/之後 f/納粹德國 nr/開始 v/改裝 v/集中營 b/為 zg/滅絕營 n/， x/以 p/貝爾 nr/澤克 nr/集中營 b/、 x/索比波 nr/集中營 b/和 c/特 n/雷布林 nr/卡 n/集中營 b/三個 m/滅絕營 n/規模 n/最大 a/。 x/當時 t/成為 v/蓋世太保 nr/的 uj/猶太人 nz/課 n/科長 n/的 uj/艾 n/希曼 nrt/則 zg/安排 v/將 zg/抓獲 v/的 uj/猶太人 nz/壓上 n/列車 n/， x/直 d/送往 v/集中營 b/處死 v/。 x/正式 ad/確立 v/將 zg/猶太人 nz/消滅 v/作為 v/國家 n/政策 n/是 v/在 p/1942年 n/1月20日 n/， x/在 p/柏林 nr/西南部 f/的 uj/萬湖 ns/的 uj/一個 m/別墅 n/開會 v/， x/落實 a/了有 v/系統 n/的 uj/殺害 v/猶太人 nz/並 c/解決 v/猶太人 nz/問題 n/最終解決方案 n/， x/即 v/「 x/萬湖會議 n/」 x/。 x/一般 a/認為 v/希姆萊 nr/是 v/直接 ad/無條件 l/的 uj/將 d/猶太人 nz/處決 v/， x/但 c/實際上 d/不然 c/， x/猶太人 nz/被 p/黨衛隊 nt/吸收 v/為 zg/勞動力 n/的 uj/重要 a/組成部分 l/， x/若 c/能 v/勞動 vn/則 zg/留下 v/， x/不能 v/勞動者 n/就 d/送往 v/集中營 b/， x/1942年 n/4月 n/希姆萊 nr/也 d/下達 v/「 x/即使 c/是 v/猶太人 nz/， x/能 v/承受 v/勞動 vn/的話 u/也 d/要 v/給 p/他 r/做到 v/死 v/」 x/， x/在 p/運送 v/猶太 nz/人時 n/， x/黨衛隊 nt/也 d/要 v/將 d/能否 v/勞動 vn/的 uj/人 n/分開 v/， x/在 p/這個 r/分區 n/的 uj/過程 n/中 f/， x/軍醫 n/擁有 v/很大 a/的 uj/權限 n/， x/著名 a/的 uj/「 x/死亡天使 n/」 x/約瑟夫 nrt/· x/門 zg/格勒 nrt/就是 d/其中 r/一名 m/。 x/武裝親衛隊 n/. x/納粹黨掌權 n/後 f/， x/希姆萊 nr/急著 v/擴張 v/他 r/的 uj/黨衛隊 nt/實力 n/， x/並 c/將 d/其 r/升為 v/一個 m/軍事組織 n/。 x/雖然 c/長刀之夜 n/事件 n/事由 n/黨衛隊 nt/執行 v/而 c/受到 v/希特勒 nr/與 zg/軍方 n/的 uj/肯定 v/， x/並 c/給予 v/3 m/個團 n/軍力 n/的 uj/特務 n/部隊 n/， x/但 c/軍方 n/一直 d/在 p/遏止 v/他 r/的 uj/膨脹 v/， x/特務 n/部隊 n/在 p/戰時 t/必須 d/由 p/軍方 n/所 c/管理 vn/， x/並 c/會 v/給予 v/賬本 n/（ x/Soldbuch eng/） x/和 c/軍人 n/證件 n/（ x/Wehrpa eng/ß x/） x/。 x/特務 n/部隊 n/的 uj/訓練 vn/與 p/建 v/製 zg/得到 v/國防軍 nt/的 uj/合作 vn/， x/1934年 n/10月 n/他們 r/在 p/巴伐利亞 ns/開辦 v/一間 m/黨衛隊 nt/士官 n/學校 n/， x/隔 v/年 m/又 d/在 p/不倫瑞克 ns/開辦 v/軍校 n/， x/特務 n/部隊 n/由 p/保羅 nrt/· x/豪 n/塞爾 nrt/（ x/國防軍 nt/中將 n/， x/後來 t/的 uj/武裝親衛隊 n/上級 b/集團 n/領袖 n/） x/訓練 vn/， x/並 c/獲得 v/不少 d/成果 n/。 x/1936年 n/10月1日 n/， x/希姆萊 nr/任命 n/豪 n/塞爾 nrt/為 zg/特務 n/部隊 n/總監 n/。 x/希姆萊 nr/又 d/將 d/管理 vn/集中營 b/的 uj/骷髏 n/隊 n/與 p/警察 v/軍事化 n/訓練 vn/， x/成為 v/後來 t/的 uj/骷髏 n/師 ng/與 p/警察 v/師 zg/。 x/1939年 n/5月 n/， x/希特勒 nr/允許 v/希姆萊 nr/組建 v/兵員 n/限定 v/2 m/萬 m/人 n/的 uj/黨衛隊 nt/特務 n/部隊 n/師團 n/， x/但 c/希姆萊 nr/急欲 i/組建 v/有 v/砲 zg/兵 n/的 uj/部隊 n/。 x/1940年 n/4月22日 n/， x/親衛隊 n/特務 n/部隊 n/被 p/改名 v/為 zg/武裝親衛隊 n/。 x/在 p/希特勒 nr/完全 ad/控制 v/國防軍 nt/後 f/， x/武裝親衛隊 n/就 d/已有 v/3 m/個師 n/， x/每個 r/師 zg/配有 v/1 m/個 q/砲 yg/兵營 n/。 x/這時 r/希姆萊 nr/共 d/擁有 v/： x/第一 m/阿道夫 nr/· x/希特勒 nr/警衛 n/旗隊 n/裝甲師 n/、 x/預備 v/師 zg/（ x/後來 t/的 uj/帝 n/國師 n/） x/、 x/骷髏 n/師 ng/和 c/警察 v/師 zg/， x/人數 n/約 d/有 v/10 m/萬 m/人 n/。 x/在 p/大戰 nz/期間 f/武裝親衛隊 n/的 uj/規模 n/不斷擴大 n/， x/最高峯 n/約 d/有 v/38 m/個師 n/， x/共 n/90 m/萬 m/人 n/之 u/多 m/。 x/和 c/國防軍 nt/相比 v/， x/親衛隊 n/的 uj/陣亡 v/與 zg/受傷 v/比率 n/較 d/低 a/， x/希姆萊 nr/也 d/因此 c/對 p/國防軍 nt/人員 n/說 zg/「 x/可以 c/將 d/較 d/困難 an/的 uj/任務 n/交給 v/親衛隊 n/」 x/。 x/親衛隊 n/的 uj/兵員 n/募集 v/是 v/由 p/親衛隊 n/本部 r/的 uj/長官 n/戈特 nrt/萊柏 ns/· x/貝爾格 nr/（ x/Gottlob eng/  x/Berger eng/） x/所 c/主導 b/， x/希姆萊 nr/於 nr/各地 r/招兵買馬 i/和 c/募集 v/人力 n/， x/包括 v/希特拉青年團 n/（ x/後來 t/的 uj/希特拉青年團 n/裝甲師 n/） x/、 x/各 r/地誌 n/願者 n/組成 v/的 uj/外籍 n/軍團 n/， x/使得 v/非 h/德語系 n/國家 n/士兵 n/開始 v/混入 v/親衛隊 n/（ x/甚至 d/有 v/穆斯林 nz/、 x/克羅 nrt/埃西亞 n/和 c/印度人 nz/， x/突厥人 n/） x/， x/並 c/主打 n/「 x/反共 d/十字軍 n/」 x/的 uj/口號 n/吸引 v/青年 t/加入 v/親衛隊 n/， x/大戰 nz/結束 v/時 zg/組成 v/為 zg/： x/一半 m/是 v/德國人 nr/， x/四分之一 mq/是 v/德 j/裔 nr/外國人 n/， x/四分之一 mq/是 v/外國人 n/。 x/隨著 v/戰爭 n/的 uj/失利 vn/， x/希特勒 nr/開始 v/不 d/信任 n/國防軍 nt/， x/而 c/將 d/勝利 vn/的 uj/希望 v/寄託 v/在 p/武裝親衛隊 n/上 f/， x/在 p/希特勒 nr/暗殺 v/計劃 n/鎮壓 v/後 f/， x/希姆萊 nr/被 p/授予 v/了 ul/國內 s/預備 v/軍 zg/司令官 nr/的 uj/職務 n/（ x/實際上 d/由 p/親衛隊 n/上級 b/集團 n/領袖 n/漢斯 nz/· x/屈 nr/特納 nrt/（ x/Hans eng/  x/J x/ü x/ttner eng/） x/指揮 v/） x/， x/這時 r/由 p/陸軍 nr/武器 n/局所 n/生產 vn/的 uj/V2火箭 n/被 p/轉手 v/至 p/親衛隊 n/經濟 n/管理局 n/控制 v/， x/親衛隊 n/對 p/國防軍 nt/有 v/了 ul/絕對 d/的 uj/優勢 n/。 x/1944年 n/12月10日 n/， x/希姆萊 nr/擔任 v/上萊茵 n/集團軍 n/司令官 nr/， x/但 c/在 p/阿登 nrt/攻勢 n/小幅 n/進展 vn/後 f/， x/就 d/受到 v/蒙哥馬利 ns/的 uj/阻擊 v/後 f/潰敗 v/。 x/1945年 n/1月25日 n/， x/希姆萊 nr/又 d/被 p/任命 n/為 zg/維斯瓦河 ns/集團軍 n/（ x/Army eng/  x/Group eng/  x/Vistula eng/） x/司令官 nr/， x/迎擊 v/進逼 v/德國 ns/本土 n/的 uj/蘇聯紅軍 nt/， x/但 c/沒有 v/作戰經驗 n/的 uj/希姆萊 nr/根本 a/不會 v/指揮 v/， x/在 p/一 m/波 ns/小型 b/攻勢 n/後 f/集團軍 n/就 d/被 p/擊潰 v/了 ul/， x/希姆萊 nr/還 d/因此 c/以 p/裝病 n/逃離 v/現實 n/， x/參謀總長 n/古德里安 nrt/勸 v/希特勒 nr/將 zg/希姆萊 nr/指揮權 n/交出來 l/， x/希特勒 nr/對 p/希姆萊 nr/仍 d/有 v/信心 n/而 c/拒絕 v/。 x/不久 a/後 f/， x/武裝親衛隊 n/因為 c/蘇軍 nr/即將 d/包圍 v/他們 r/而 c/倉皇 n/撤離 v/， x/希特勒 nr/見狀 n/以 p/電話 n/對 p/希姆萊 nr/大 a/罵 v/： x/「 x/希姆萊 nr/， x/武裝親衛隊 n/第 m/6 m/裝甲 b/軍 n/在 p/巴拉頓湖 ns/的 uj/失敗 v/， x/是 v/我 r/生平 n/最感 a/失望 v/的 uj/事 n/。 x/你 r/的 uj/親衛隊 n/、 x/骷髏 n/部隊 n/、 x/甚至 d/是 v/警衛 n/旗隊 n/都 d/在 p/敵人 n/面前 f/逃竄 v/， x/這 r/是 v/恥辱 n/的 uj/背叛 v/！ x/我 r/下令 v/， x/收回 v/該 r/部隊 n/軍官 n/的 uj/所有 b/勳章 n/… x/… x/」 x/， x/希姆萊 nr/對此 d/反駁 v/道 q/： x/「 x/我 r/的 uj/元首 t/， x/要 v/我 r/摘去 v/軍官 n/們 k/的 uj/勳章 n/， x/那 r/我 r/也 d/只能 v/去 v/巴拉頓湖 ns/上 f/的 uj/死屍 n/拿 v/。 x/我 r/的 uj/元首 t/， x/要是 c/真的 d/這麼 r/做 v/， x/就算 v/是 v/一個 m/普通 nz/黨衛隊 nt/隊員 n/也 d/不會 v/再 d/向 p/您 r/宣示 v/效命 n/了 ul/！ x/」 x/希特勒 nr/在 p/他 r/說完 v/前 f/就 d/掛 v/了 ul/電話 n/， x/該 r/集團軍 n/由 p/古德里安 nrt/說服 v/希姆萊 nr/， x/1945年 n/3月20日 n/改 v/交由 n/哥特 ns/哈德 nrt/· x/海因裏希 nrt/指揮 v/。 x/7月20日密謀案 n/. x/1944年 n/7月20日 n/， x/陸軍上校 nt/克勞斯 nrt/· x/馮 nr/· x/施陶芬貝格 nr/與 zg/國防軍 nt/多名 m/將領 n/引爆 v/炸彈 n/企圖 n/謀殺 v/希特勒 nr/， x/扶持 v/影子政府 n/奪權 v/， x/雖然 c/事後 t/希姆萊 nr/為 zg/鎮壓 v/該 r/事件 n/當事人 n/的 uj/主要 b/角色 n/， x/但 c/他 r/可能 v/也 d/有 v/涉入 v/密謀 n/案 ng/中 f/。 x/暗殺 v/計劃 n/實行 v/前 f/的 uj/1944年 n/7月17日 n/， x/蓋世太保 nr/曾 d/接獲 v/到 v/卡爾 nrt/· x/弗德 nrt/里 f/希 v/· x/戈瑞 nr/德爾 ns/（ x/Carl eng/  x/Friedrich eng/  x/Goerdeler eng/） x/和 c/路德 nrt/維格 nrt/· x/貝克 nr/上將 n/企圖 n/刺殺希特勒 n/的 uj/消息 n/， x/並 c/要求 v/希姆萊 nr/發佈 v/他們 r/的 uj/逮捕令 n/， x/但 c/希姆萊 nr/不知 v/為 zg/何原因 nr/而 c/拒絕 v/了 ul/。 x/其後 c/希姆萊 nr/命令 n/帝國 n/保安 nz/部 q/的 uj/恩斯特 nrt/· x/卡爾 nrt/滕 nr/布倫納 nr/進行 v/大規模 b/的 uj/政治 n/搜捕 v/， x/約 d/有 v/5000 m/人 n/被 p/處死 v/、 x/數千 m/人 n/被 p/關到 v/集中營 b/， x/是 v/繼 v/「 x/長刀之夜 n/」 x/以來 f/最大 a/的 uj/政治 n/清算 v/行動 vn/。 x/因為 c/此 r/事件 n/使得 v/希特勒 nr/幾乎 d/不再 d/相信 v/國防軍 nt/， x/改 v/轉而 c/信任 n/黨衛隊 nt/， x/也 d/是 v/造成 v/之後 f/希姆萊 nr/被 p/任命 n/為 zg/集團軍 n/司令 n/的 uj/主要 b/原因 n/。 x/被 p/解任 v/. x/1945年 n/春 tg/， x/希姆萊 nr/已經 d/看清 v/德國 ns/即將 d/戰敗 n/的 uj/事實 n/， x/認為 v/如果 c/要 v/自保 vn/， x/必須 d/要 v/除去 t/希特勒 nr/並 c/和 c/盟軍 n/談 v/和 c/， x/他 r/與 p/專屬 n/醫生 n/斐 j/力克斯 nz/· x/克斯 nrt/坦 n/（ x/Felix eng/  x/Kersten eng/） x/和 c/瓦爾特 nrt/· x/施倫堡 nr/（ x/Walther eng/  x/Schellenberg eng/） x/一直 d/在 p/企圖 n/透過 v/瑞士 ns/與 zg/瑞典 ns/中間人 n/的 uj/方式 n/聯絡 n/上 f/伯納德 nr/· x/勞 vn/· x/蒙哥馬利 ns/或 c/艾森豪 nr/， x/但 c/都 d/被 p/告知 v/只 d/接受 v/全面 n/的 uj/投降 v/。 x/在 p/與 p/艾森豪 nr/代表 n/接洽 v/的 uj/過程 n/中 f/， x/消息 n/被 p/英國 ns/外相 n/安東尼 nr/· x/艾登 nr/知道 v/， x/並 c/被 p/他 r/轉告 v/給 p/路透社 n/記者 n/， x/24小時 n/後 f/英美 ns/德 ns/都 d/知道 v/此事 r/了 ul/， x/希特勒 nr/對 p/不 v/在場 v/的 uj/希姆萊 nr/痛罵一頓 i/後 f/下令 v/逮捕 v/他 r/， x/但 c/希姆萊 nr/以 p/帶領 v/武裝親衛隊 n/轉向 v/北部 f/支援 v/柏林 nr/的 uj/戰鬥 vn/為 zg/理由 n/， x/早已 d/遠離 v/了 ul/柏林 nr/， x/希特勒 nr/無法 n/抓到 v/他 r/。 x/希特勒 nr/之後 f/向 p/女祕書 n/立 v/好 a/遺囑 n/， x/解除 v/希姆萊 nr/所有 b/職務 n/， x/隔日 b/舉槍 n/自殺 v/。 x/由於 c/當時 t/傳達 v/機制 n/混亂 a/， x/希姆萊 nr/的 uj/職務 n/解除 v/命令 n/並未 d/傳開 v/， x/但 c/鄧尼茲 nr/已 d/立刻 d/接受 v/帝國 n/總統 n/的 uj/職位 n/。 x/希姆萊 nr/之後 f/透過 v/收音機 n/廣播 vn/知道 v/鄧尼茲 nr/已成 v/了 ul/武裝力量 l/最高領導人 n/， x/而 c/希特勒 nr/對 p/他 r/的 uj/安排 v/讓 v/他 r/非常 d/生氣 n/， x/他 r/在 p/前往 t/鄧尼茲 nr/「 x/弗倫斯堡政府 n/」 x/途中 s/有 v/想要 v/立個 b/戰後 t/政府 n/和 c/新黨 n/— x/民族 n/集中 v/黨 n/， x/當 p/他 r/抵達 v/鄧尼茲 nr/石勒蘇益格 n/- x/荷爾斯泰因 n/的 uj/海軍 n/學校 n/後 f/要求 v/會見 n/鄧尼茲 nr/。 x/鄧尼茲 nr/因為 c/對 p/希姆萊 nr/有些 r/忌憚 v/， x/他 r/安排 v/幾隊 m/全副武裝 n/的 uj/潛艇 n/突擊隊員 n/埋伏 v/於 nr/走 v/道 q/和 c/院子 n/， x/並 c/在 p/自己 r/辦公桌 n/堆疊 v/的 uj/文件 n/下 f/放置 v/一把 m/上膛 n/的 uj/手槍 n/。 x/希姆萊 nr/與 p/他 r/會面 n/後 f/， x/以 p/畢恭畢敬 i/的 uj/態度 n/請求 v/幫 v/他 r/安排 v/一個 m/位子 n/， x/說 zg/“ x/請 v/讓 v/我 r/在 p/您 r/的 uj/帝國 n/裏 f/擔當 v/第 m/2 m/號 m/人物 n/吧 y/” x/， x/而 c/鄧尼茲 nr/則 d/直接 ad/拒絕 v/了 ul/， x/說 zg/「 x/我 r/的 uj/政府 n/只 d/接受 v/政治 n/清白 a/的 uj/人 n/」 x/， x/在 p/希姆萊 nr/一直 d/企圖 n/說服 v/後 nr/仍 d/不 d/退讓 v/， x/希姆萊 nr/就 d/失望 v/的 uj/離開 v/了 ul/。 x/5月5日 n/， x/鄧尼茲 nr/告訴 v/他 r/所有 b/的 uj/內閣 n/成員 n/， x/蒙哥馬利 ns/要求 v/德國 ns/無條件投降 n/， x/他 r/要 v/所有 b/成員 n/對此 d/發表意見 l/。 x/希姆萊 nr/提議 v/道 q/： x/「 x/我們 r/應該 v/避免 v/位於 v/丹麥 ns/和 c/挪威 ns/的 uj/德軍 nr/向 p/蘇軍 nr/投降 v/」 x/， x/並 c/要求 v/以 p/他 r/的 uj/一名 m/部下 n/作為 v/談判代表 n/， x/理由 n/是 v/希姆萊 nr/認為 v/在 p/北歐 ns/的 uj/德軍 nr/會 v/是 v/將來 t/用作 v/戰後 t/談判 vn/的 uj/籌碼 n/， x/以此 c/作為 v/維持 v/戰後 t/德國 ns/治安 ns/的 uj/武裝力量 l/。 x/希姆萊 nr/這個 r/提議 v/被 p/接受 v/了 ul/， x/但 c/鄧尼茲 nr/實在 v/是 v/很 d/討厭 v/希姆萊 nr/， x/且 c/他 r/在 p/盟軍 n/的 uj/角色 n/也 d/非常 d/惹 v/人 n/嫌惡 n/， x/鄧尼茲 nr/在 p/5月6日 n/下達 v/一道 m/命令 n/給 p/了 ul/希姆萊 nr/： x/被俘 v/與 zg/死亡 v/. x/為了 p/避人耳目 i/， x/希姆萊 nr/與 zg/隨行 d/裝扮成 l/交通警察 n/， x/還 d/露宿街頭 n/， x/希姆萊 nr/以 p/黑 n/布條 n/遮住 v/左眼 m/， x/穿著 n/深色 n/平民 n/上衣 n/， x/使用 v/別人 r/的 uj/護照 n/， x/名叫 v/： x/海因裏希 nrt/· x/希特 nrt/爾 nr/齊格爾 nr/（ x/該人 r/已 d/被 p/希特勒 nr/在 p/720 m/事件 n/中 f/處決 v/） x/， x/在 p/5月21日 n/前往 t/不來梅港 ns/時 ng/被 p/英軍 j/檢查哨 n/攔 v/下來 t/， x/因為 c/其 r/身份證 n/太 d/新 a/了 ul/， x/不 d/像是 v/逃難 v/者 k/所 u/擁有 v/的 uj/而 c/被 p/拘留 v/。 x/5月23日 n/下午 t/2 m/點 m/， x/希姆萊 nr/和 c/另外 c/2 m/人 n/要求 v/見面 n/負責 v/軍官 n/， x/希姆萊 nr/當場 s/解除 v/便裝 v/表示 v/身份 n/， x/要求 v/見 v/蒙哥馬利 ns/元帥 nr/。 x/在 p/等待 v/回覆 v/時 zg/， x/英軍 j/的 uj/一個 m/上尉 n/拿 v/了 ul/一套 m/英國 ns/軍服 n/要 v/給 p/希姆萊 nr/穿 zg/， x/但 c/他 r/拒絕 v/了 ul/。 x/之後 f/上尉 n/命令 n/士兵 n/對 p/他 r/搜身 v/， x/發現 v/在 p/他 r/上衣 n/襯 v/中 f/有 v/一瓶 m/毒藥 n/， x/此外 c/沒 v/別的 r/。 x/在 p/經過 p/洗澡 v/和 c/吃飯 v/後 f/， x/晚上 t/蒙哥馬利 ns/情報部門 n/負責人 n/麥克爾 nr/· x/莫菲 nr/上校 j/決定 v/由 p/情報人員 n/押送 v/希姆萊 nr/至 p/第2軍 n/團 n/總部 n/。 x/抵達 v/總部 n/後 f/， x/莫菲問 nr/上尉 n/是否 v/檢查 vn/他 r/藏有 b/毒藥 n/， x/另外 c/要求 v/醫生 n/檢查 vn/他 r/嘴中 s/， x/在 p/嘴中 s/發現 v/一枚 m/氰化鉀 nz/膠囊 n/， x/在 p/醫生 n/想要 v/更 d/一步 m/確認 v/後 f/， x/希姆萊 nr/突然 ad/轉頭 v/咬破 v/膠囊 n/， x/自殺 v/而 c/死 v/。 x/1 m/天 q/後 nr/， x/接到 v/英軍 j/報告 n/前來 t/的 uj/美軍 j/與 zg/蘇軍 nr/驗屍 n/後 f/， x/希姆萊 nr/被 p/葬 vg/於 nr/呂訥堡 n/郊外 s/。 x/家庭 n/. x/1928年 n/7月3日 n/， x/希姆萊 nr/與 zg/地主 n/之 u/女 b/瑪佳莉 nr/特 d/結婚 v/， x/瑪佳莉 nr/特是 d/金髮碧眼 nr/高個子 n/女性 n/， x/是 v/希姆萊 nr/心中 s/理想 n/的 uj/「 x/德意志 ns/女性 n/」 x/， x/她 r/曾 d/在 p/第一次世界大戰 nz/時 ng/做 v/過 ug/護士 n/， x/在 p/柏林 nr/結過 v/短期 b/的 uj/婚姻 n/， x/之後 f/用 p/父親 n/的 uj/資金 n/開 v/了 ul/一間 m/小 a/診所 v/。 x/可是 c/她 r/比 p/希姆萊 nr/年長 n/7 m/歲 m/， x/而且 c/是 v/基督 n/新教 nz/信徒 n/， x/天主教 nz/的 uj/希姆萊 nr/父母 n/對於 i/他們 r/倆 m/的 uj/婚事 n/極力 n/反對 d/。 x/1929年 n/8月 n/， x/希姆萊 nr/與 p/瑪佳莉 nr/特有 b/了 ul/一個 m/女兒 n/古德仁 nr/（ x/Gudrun eng/） x/， x/希姆萊 nr/非常 d/疼愛 n/古德仁 nr/， x/並稱 v/她 r/作 v/「 x/P x/ü x/ppi eng/（ x/人偶 n/） x/」 x/， x/希姆萊 nr/也 d/曾 d/帶 v/古德仁 nr/去 v/他 r/的 uj/工作 vn/場所 n/， x/包括 v/集中營 b/。 x/參觀 n/集中營 b/的 uj/當晚 t/， x/古德仁 nr/就 d/在 p/日記 n/中 f/就 d/記述 v/了 ul/。 x/另一方面 c/， x/瑪佳莉 nr/特 d/還有 v/前次 r/婚姻 n/留下 v/的 uj/一個 m/男孩子 n/， x/但 c/希姆萊 nr/對 p/這個 r/繼子 n/幾乎 d/沒有 v/關心 n/。 x/在 p/古德仁 nr/出生 v/後 f/， x/希姆萊 nr/與 p/瑪佳莉 nr/特 d/之間 f/的 uj/感情 n/越來越 d/糟 v/， x/最後 f/演變 v/成分 n/居 v/。 x/希姆萊 nr/於 nr/1937年 n/開始 v/與 p/他 r/前任 n/的 uj/女祕書 n/海德 nrt/溫 zg/· x/波 j/哈斯特 nrt/（ x/Hedwig eng/  x/Potthast eng/） x/成為 v/情人 n/關係 n/， x/希姆萊 nr/稱 v/波 ns/哈斯特 nrt/為 zg/「 x/我 r/的 uj/小白兔 nr/」 x/， x/雖然 c/他 r/試著 v/與 p/瑪佳莉 nr/特 d/離婚 v/， x/但因為 c/家庭 n/的 uj/天主教 nz/保守 v/而 c/從未 d/成功 a/， x/結果 n/波 j/哈斯特 nrt/分別 d/在 p/1942年 n/與 zg/1944年 n/先後 t/為 zg/希姆萊 nr/生 v/了 ul/2 m/個 m/孩子 n/。 x/波 j/哈斯特 nrt/的 uj/父母 n/對於 i/希姆萊 nr/既 c/沒有 v/離婚 v/再娶 v/的 uj/打算 v/， x/也 d/沒有 v/一棟 m/房子 n/能 v/給 p/他們 r/女兒 n/而 c/非常 d/不滿 a/。 x/但 c/私生活 n/非常 d/簡樸 a/的 uj/希姆萊 nr/卻 d/沒有 v/能 v/買 v/新房子 n/的 uj/積蓄 n/， x/他 r/以 p/職權 n/在 p/納粹 nr/活動 vn/中 f/祕密 n/地 uv/挪出 v/了 ul/8 m/萬馬克 nr/， x/而 c/這件 mq/事 n/被 p/馬丁 nr/· x/鮑曼 nr/知道 v/， x/但 c/他 r/沒有 v/張揚 nr/。 x/之後 f/希姆萊 nr/用 p/這筆 r/錢 n/在 p/柏希 nr/加登 v/- x/舍瑞 nz/（ x/Berchtesgaden eng/- x/Sch eng/ö x/nau eng/） x/一帶 n/蓋 v/了 ul/房子 n/， x/而 c/鮑曼 nr/家 q/就 d/在 p/附近 f/， x/鮑曼 nr/夫人 n/後來 t/與 p/波 ns/哈斯特 nrt/友好 a/， x/並 c/緩和 a/了 ul/希姆萊 nr/與 p/鮑曼 nr/的 uj/權力 n/衝突 vn/， x/促成 v/了 ul/友情 n/。 x/希姆萊 nr/父親 n/同父異母 n/哥哥 ns/科 n/南德 ns/· x/希姆萊 nr/（ x/Konrad eng/  x/Himmler eng/） x/的 uj/孫子 nr/漢斯 nz/· x/希姆萊 nr/（ x/Hans eng/  x/HImmler eng/） x/是 v/黨衛隊 nt/一名 m/中尉 n/， x/有 v/一次 m/因為 c/酒醉 v/而 c/洩漏 v/了 ul/重要 a/情報 n/， x/希姆萊 nr/知道 v/後 nr/對 p/他 r/判處死刑 i/。 x/之後 f/雖然 c/有 v/減刑 v/， x/但 c/漢斯 nz/被 p/送到 v/了 ul/前線 n/， x/之後 f/他 r/又 d/因為 c/說 v/了 ul/對 p/黨衛隊 nt/否定 v/的 uj/言論 n/， x/被 p/當作 v/「 x/男同性戀 n/」 x/而 c/送到 v/達豪集中營 n/， x/槍殺 v/處死 v/。 x/對於 i/希姆萊 nr/來說 u/， x/即使 c/有 v/血緣關係 l/， x/只要 c/有 v/破壞 v/紀律 n/的 uj/行為 n/也 d/不會 v/被 p/寬恕 v/。 x/戰後 t/， x/由於 c/希姆萊 nr/一 m/姓 v/被 p/視為 v/「 x/邪惡 a/的 uj/代名詞 n/」 x/， x/古德仁 nr/之後 f/就 d/改姓 v/為 zg/貝爾維 n/茲 rg/（ x/Burwitz eng/） x/， x/並為 c/黨衛隊 nt/隊員 n/的 uj/法律 n/辯護 v/， x/也 d/是 v/歷史修正主義 n/的 uj/重要 a/成員 n/。 x/他 r/還有 v/一位 m/姪 zg/孫女 nr/卡特琳 nrt/· x/希姆萊 nr/， x/是 v/他 r/弟弟 n/恩斯特 nrt/· x/希姆萊 nr/的 uj/孫女 nr/， x/是 v/一位 m/作家 n/。 x/神祕學 l/. x/希姆萊 nr/對於 i/一些 m/可能 v/具有 v/神祕力量 n/的 uj/物品 n/或 c/魔術 n/感到 v/著迷 v/， x/信奉 v/神祕主義 n/與 zg/北歐 ns/宗教 n/， x/曾 d/成立 v/組織 v/德意志 ns/研究會 n/（ x/Ahnenerbe eng/） x/前往 t/世界各地 l/， x/包括 v/西藏 ns/和 c/黑森林 nr/等 u/地 uv/尋找 v/所有 b/有關 vn/「 x/亞利安 ns/超人 n/」 x/的 uj/祖先 nr/和 c/聖杯 n/的 uj/蹤影 n/， x/還 d/對 p/占星術 nr/、 x/黑 a/魔術 n/成立 v/組織 v/研究 vn/。 x/希姆萊 nr/還 d/聯繫 n/與 zg/贊助 v/歷史 n/久遠 d/的 uj/組織 v/— x/維利 nz/會 v/， x/試圖 v/找出 v/神祕力量 n/「 x/維利 nz/」 x/（ x/Vril eng/） x/獲得 v/戰爭 n/的 uj/勝利 vn/（ x/包括 v/卐 yg/字 n/標誌 n/、 x/UFO eng/圓盤 n/型 k/飛行器 n/與 p/V eng/型 k/飛彈 n/被 p/一些 m/神祕學 l/學者 n/認為 v/是 v/納粹黨 nr/與 zg/維利 nz/會 v/的 uj/關係 n/衍生 v/而 c/成 v/的 uj/） x/， x/以及 c/進行 v/種族 n/淨化 n/， x/讓 v/「 x/日耳曼 ns/之 u/血 n/」 x/在 p/歐洲 ns/廣佈 ns/， x/納粹德國 nr/許多 m/重要 a/的 uj/核心人物 l/都 d/是 v/維利 nz/會 v/的 uj/成員 n/， x/包括 v/希特勒 nr/、 x/戈林 nr/、 x/希姆萊 nr/、 x/魯道夫 nr/· x/赫斯 nr/、 x/阿爾弗雷德 n/· x/羅森堡 nr/和 c/馬丁 nr/· x/鮑曼 nr/。 x/希姆萊 nr/還有 v/以 p/1300 m/萬 m/帝國 n/馬克 nr/改建 v/古堡 ns/維 zg/威爾斯 nrt/堡 ng/（ x/Wewelsburg eng/） x/， x/作為 v/自己 r/的 uj/別墅 n/， x/建立 v/地下室 n/， x/專門 n/施行 v/魔術 n/、 x/儀式 n/， x/還有 v/地方 n/專門 n/放置 v/永久 d/黨衛隊 nt/的 uj/遺骨 n/， x/以及 c/黨衛隊 nt/最高 a/法庭 n/、 x/希姆萊 nr/的 uj/會客室 n/、 x/多達 nrt/12000 m/冊 q/藏書 nz/的 uj/圖書 n/間 f/。 x/希姆萊 nr/雖然 c/為 zg/希特勒 nr/前來 t/做 v/了 ul/很多 m/功夫 n/準備 v/迎接 v/， x/但 c/希特勒 nr/直到 v/戰爭 n/結束 v/都 d/不曾 d/來 v/過 ug/。 x/參考文獻 n/. x/書目 n/. x/中文 nz/書目 n/日文 n/書目 n/英文 nz/書目 n', 'id': 119285, 'url': 'https://zh.wikipedia.org/wiki?curid=119285'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BsgJYRLnM-bk"},"source":["# 製作要丟進rdd的檔案\n","\n","counter = 0\n","fo = open(\"IDTitle_content_string.txt\", \"w\")\n","\n","typeFilter = ('x', 'nrt', 'nz', 'm', 'eng', 'd', 'a', 'v')\n","wordFilter = ('，', ',', \"。\", \"(\", \"（\", \")\", \"）\", \"「\", \"」\", \"．\", \"·\", \"《\", \"》\", \"、\", \".\", \"的\", \"和\",\n","        \"＂\", \"\\\"\", \"“\", \"是\", \"被\", \"在\", \"將\", \"由\", \"他\", \"她\", \"它\", \"有\", \"於\", \"而\", \"；\", \"便\",\n","        \"就\", \"能\", \"及\", \"以\", \"中\", \"因\", \"稱\", \"稱為\", \"：\", \"我\", \"你\", \"妳\", \"與\", \"人\", \"也\", \"之\",\n","        \"這\", \"那\", \"”\", \"已\", \"已經\", \"所\", \"不\", \"上\", \"才\", \"此\", \"為\", \"會\", \"未\", \"卻\", \"第\", \"都\",\n","        \"或\", \"很\", \"後\", \"前\", \"-\", \"]\", \"[\", \"+\", \"*\", \"{\", \"}\", \"‘\", \"’\", \":\", \"之下\", \"之中\", \"之上\",\n","        \"只是\", \"只\", \"非\", \"如下\", \";\", \"了\", \"從\", \"並\", \">\", \"<\", \"＞\", \"＜\", \"為\", \"\", \"／\", \"/\", \"\\\\\",\n","        \"等\", \"時\", \"對\", \"至\" ,\"則\", \"可以\", \"下\", \"又\", \"更\", \"可\", \"們\", \"爲\")\n","\n","for obj in objects:\n","  # output格式：\"ID:TITLE WORD1 WORD2 WORD3 ...\"\n","  outS = str(obj['id']) + ':' + obj['title'] + ' '\n","  s = obj['content'].split()\n","  for i in range(len(s)-1, 0, -1):\n","    pos = s[i].find('/')\n","    # 基本上都找得到。避免任何例外，找不到的直接移除。\n","    if pos != -1:\n","      wordType = s[i][0:pos]\n","      word = s[i][pos+1:].strip(\"(|)|,|'|\\\"\")\n","      if not word:\n","        # strip後字就消失了的話，理論上不會有這個情況\n","        s.pop(i)\n","        continue\n","      # wordType = \"n\\\", \"x\\\"之類的東西\n","      # word = \"劉備\", \"關羽\"等等\n","      if wordType in typeFilter:\n","        s.pop(i)\n","      elif word in wordFilter:\n","        s.pop(i)\n","      else:\n","        #s[i] = word\n","        outS += word\n","        outS += ' '\n","    else:\n","      s.pop(i)\n","  outS += '\\n'\n","  # 寫回檔案\n","  fo.write(outS)\n","\n","  # 跑一點output讓我安心\n","  counter += 1\n","  if (counter == 100000):\n","    print('100000筆')\n","    counter = 0\n","fo.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mg-MlkvlNouH"},"source":["# 可見測試結果\n","!head IDTitle_content_string.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z5Oye6nhQBsR","executionInfo":{"status":"ok","timestamp":1604578661249,"user_tz":-480,"elapsed":636,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}},"outputId":"6a553601-b3fb-483d-d3eb-a8c0d5ce1359","colab":{"base_uri":"https://localhost:8080/"}},"source":["#看看能不能丟進rdd\n","\n","#讀取兩個資料\n","rdd = sc.union([sc.textFile(file_dir + \"new_PTT_IDTitle_content_string.txt\"), sc.textFile(file_dir + \"IDTitle_content_string.txt\")])\n","rdd.take(2)"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1588950762 華視新聞 無極限 說謊 規定 條 法 主動 訊息 隻手遮天 選委會 地方 連頭 行動 韓 為了 主委 中選會 更何況 法條 辦理 規定 日 投票 員工 區內 選舉 本來 投票權 區 選舉 戶籍 規定 常識 知識 高雄市議員 身 黃捷 孫大千 選民 民意代表 樣 聲量 韓 假消息 利用 怒轟黃捷 職守 民意代表 做 功夫 訊息 民眾 高雄市議員 身 黃捷 批評 發文 臉書 下午 孫大千 立委 此事 針對 黃捷 批 怒 書 臉 孫大千 辦理 規定 勞動部 事項 工資 勞工 勞基法 理 辦 規定 日 投票 員工 區內 選舉 投票權 區 選舉 戶籍 日均 投票 前往 員工 機構 事業 團體 學校 機關 各級 區內 選舉 公告 中選會 一致 打電話 每個人 國定假日 即為 當天 高雄市 設籍 民眾 人員 選委會 選委會 高雄市 網 華視新聞 國定假日 高雄 設籍 民眾 選委會 高雄 消息 選委會 高雄市 黃捷怒 範疇 日 投票 選舉 地區性 這類 選舉 選舉 有分 選舉 選委會 高雄市 沒想 投票 回來 上班 需 當天 國定假日 當天 日 投票 詢問 選委會 高雄市 陳情 民眾 高雄 戶籍 工作 縣市 發文 臉書 今在 黃捷 範圍 假 日 投票 韓 選委會 高雄市 爆料 黃捷 道歉 公開 黃捷 聲量 韓 假消息 利用 黃捷 怒 孫大千 立委 國定假日 投票 新聞稿 火速 中選會 消息 選委會 高雄市 範疇 投票 區域 投票 韓 登場 下月 說 選委會 高雄市 陳情 民眾 上午 日 今 黃捷 高雄市議員 時代力量 高雄市 綜合 庭 徐筠 聲量 韓 假消息 利用 怒轟 孫大千 黃捷 假消息 利用 怒轟 孫大千 黃捷',\n"," '1588950829 短 道路 國家利益 維護 條 法國 澳洲 日本 歐洲國家 美國 國家 經貿關係 優先發展 國家 規範 體系 價值觀 本能 外交政策 法國 說 外交官 名 法國 中國 戰是 狀況 法國 改變 危險關係 病毒 肺炎 武漢 實驗室 武漢 所有 中國 技 國家 價值觀 尊重 經濟 問責 中國 說 法國外交官 法國 核武 研發 他國 難免會 理論 元素 鈽 基礎設施 用途 軍事 嚴格來說 工廠 核 迎頭趕上 中國 合理性 技術 中國 法國外交官 國家 沿線 一帶一路 出口 地方 國 電廠 低成本 希望 中國 專家 方案 這項 延宕 雙邊 法 立場 法國外交部 風險 專家 項目 兩用 軍民 核廢料 計畫 合作 歐元 達 規模 雖是 方案 工廠 核 中國 歐安 公司 法國核能 說 前車之鑑 實驗室 合作 技術 知識 改革開放 藉由 國家 工廠 世界 中國 企圖 技術 西方 手段 藉 中國 規 法 合作 實驗室 軍事 民用 並於 中央軍民融合發展委員會 領導 平 國家主席 中國 藩籬 之間 技術 和民 軍用 正式 中國 能力 關鍵 中國 空中巴士 考量 財務 工廠 鏡子 製造 中國人 當時 表示遺憾 技術 掠奪 中國 利益 經濟 短期 空中巴士 巨擘 歐洲航空 人士 法籍 北京 用途 軍事 單位 研究 類型 北京 懷疑 法國 說 中國 實驗室 武漢 複製技術 出口 技術 外國 複製 全國 技術 外國 消化 技術 外國 面 張貼 入口處 核電廠 反應爐 壓水式 國 法 核電廠 台山 中國 說 中國 移轉 技術 棘手 法國 令 技術 兩用 軍民 出售 中華人民共和國 共產黨 為何 問題 法國 目前 爆發 疫情 肺炎 武漢 險境 法國 反使 技術 這種 國家 無止境 發展 軍力 工具 生化武器 研發 中國 日後 複製 抄襲 中國人 說 法國外交官 名 不具 用途 軍事 單位 研究 類型 北京 懷疑 法國 說 中國 實驗室 武漢 歷史 冠狀病毒 重寫 日期 研究 國家 黨 以來 疫情 這次 領導 共產黨 獨立 科學 國家 國家 列寧主義 中國 忘了 倪雅玲 正常國家 中國 開放 資本主義 人員 研究 現實 中國制度 科學界 整個 當年 表 倪雅玲 負責人 部門 亞洲 基金會 研究 戰略 法國智庫 實驗室 監控 研究員 中方 培訓 法方 原本 計畫 合作 國 兩 實驗室 徹底 法國人 揭幕典禮 盛大 實驗室 武漢 法國人 開放 民主思想 進化 政權 以為 外交 象 形 對外 官方 中國 法國 世界貿易組織 中國 正值 當時 堅決 細菌武器 製造 中共 擔心 外交部 對外安全總局 法國 法國國防部 傳染病 抵抗 中國 疫情 症候群 呼吸道 急性 幫助 主張 科學家 政壇 技術 中國 倡議 哈發林 總理 哈克 席 法國總統 實驗室 安全 生物 國家 武漢 學院 科 中國 全名是 實驗室 武漢 調查報告 實驗室 武漢 針對 塞爾 拉 專家 戰略 防務 日刊 報 費加洛 法國 電台 國際廣播 法國 正常國家 民主 成 變 進化 中共政權 以為 實驗室 徹底 法方 交易 實驗室 兩國 法國媒體 冠狀病毒 洩 外 實驗室 武漢 中法 日電 台北 中央社 真面目 中共 法國 實驗室 武漢 媒 法 儒 楊昇 帆 曹宇 編輯 中共 法國 實驗室 武漢 媒 法']"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"G5o_QJ2eQIj1"},"source":["# 準備製作totalWordCount.txt"]},{"cell_type":"code","metadata":{"id":"I2DmngSKTFuC","executionInfo":{"status":"ok","timestamp":1604578669217,"user_tz":-480,"elapsed":776,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}},"outputId":"52aea40b-bebe-493c-e9db-9f6f7ebc6ce0","colab":{"base_uri":"https://localhost:8080/"}},"source":["rdd = sc.union([sc.textFile(file_dir + \"new_PTT_IDTitle_content_string.txt\"), sc.textFile(file_dir + \"IDTitle_content_string.txt\")])\n","rdd.take(2)"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1588950762 華視新聞 無極限 說謊 規定 條 法 主動 訊息 隻手遮天 選委會 地方 連頭 行動 韓 為了 主委 中選會 更何況 法條 辦理 規定 日 投票 員工 區內 選舉 本來 投票權 區 選舉 戶籍 規定 常識 知識 高雄市議員 身 黃捷 孫大千 選民 民意代表 樣 聲量 韓 假消息 利用 怒轟黃捷 職守 民意代表 做 功夫 訊息 民眾 高雄市議員 身 黃捷 批評 發文 臉書 下午 孫大千 立委 此事 針對 黃捷 批 怒 書 臉 孫大千 辦理 規定 勞動部 事項 工資 勞工 勞基法 理 辦 規定 日 投票 員工 區內 選舉 投票權 區 選舉 戶籍 日均 投票 前往 員工 機構 事業 團體 學校 機關 各級 區內 選舉 公告 中選會 一致 打電話 每個人 國定假日 即為 當天 高雄市 設籍 民眾 人員 選委會 選委會 高雄市 網 華視新聞 國定假日 高雄 設籍 民眾 選委會 高雄 消息 選委會 高雄市 黃捷怒 範疇 日 投票 選舉 地區性 這類 選舉 選舉 有分 選舉 選委會 高雄市 沒想 投票 回來 上班 需 當天 國定假日 當天 日 投票 詢問 選委會 高雄市 陳情 民眾 高雄 戶籍 工作 縣市 發文 臉書 今在 黃捷 範圍 假 日 投票 韓 選委會 高雄市 爆料 黃捷 道歉 公開 黃捷 聲量 韓 假消息 利用 黃捷 怒 孫大千 立委 國定假日 投票 新聞稿 火速 中選會 消息 選委會 高雄市 範疇 投票 區域 投票 韓 登場 下月 說 選委會 高雄市 陳情 民眾 上午 日 今 黃捷 高雄市議員 時代力量 高雄市 綜合 庭 徐筠 聲量 韓 假消息 利用 怒轟 孫大千 黃捷 假消息 利用 怒轟 孫大千 黃捷',\n"," '1588950829 短 道路 國家利益 維護 條 法國 澳洲 日本 歐洲國家 美國 國家 經貿關係 優先發展 國家 規範 體系 價值觀 本能 外交政策 法國 說 外交官 名 法國 中國 戰是 狀況 法國 改變 危險關係 病毒 肺炎 武漢 實驗室 武漢 所有 中國 技 國家 價值觀 尊重 經濟 問責 中國 說 法國外交官 法國 核武 研發 他國 難免會 理論 元素 鈽 基礎設施 用途 軍事 嚴格來說 工廠 核 迎頭趕上 中國 合理性 技術 中國 法國外交官 國家 沿線 一帶一路 出口 地方 國 電廠 低成本 希望 中國 專家 方案 這項 延宕 雙邊 法 立場 法國外交部 風險 專家 項目 兩用 軍民 核廢料 計畫 合作 歐元 達 規模 雖是 方案 工廠 核 中國 歐安 公司 法國核能 說 前車之鑑 實驗室 合作 技術 知識 改革開放 藉由 國家 工廠 世界 中國 企圖 技術 西方 手段 藉 中國 規 法 合作 實驗室 軍事 民用 並於 中央軍民融合發展委員會 領導 平 國家主席 中國 藩籬 之間 技術 和民 軍用 正式 中國 能力 關鍵 中國 空中巴士 考量 財務 工廠 鏡子 製造 中國人 當時 表示遺憾 技術 掠奪 中國 利益 經濟 短期 空中巴士 巨擘 歐洲航空 人士 法籍 北京 用途 軍事 單位 研究 類型 北京 懷疑 法國 說 中國 實驗室 武漢 複製技術 出口 技術 外國 複製 全國 技術 外國 消化 技術 外國 面 張貼 入口處 核電廠 反應爐 壓水式 國 法 核電廠 台山 中國 說 中國 移轉 技術 棘手 法國 令 技術 兩用 軍民 出售 中華人民共和國 共產黨 為何 問題 法國 目前 爆發 疫情 肺炎 武漢 險境 法國 反使 技術 這種 國家 無止境 發展 軍力 工具 生化武器 研發 中國 日後 複製 抄襲 中國人 說 法國外交官 名 不具 用途 軍事 單位 研究 類型 北京 懷疑 法國 說 中國 實驗室 武漢 歷史 冠狀病毒 重寫 日期 研究 國家 黨 以來 疫情 這次 領導 共產黨 獨立 科學 國家 國家 列寧主義 中國 忘了 倪雅玲 正常國家 中國 開放 資本主義 人員 研究 現實 中國制度 科學界 整個 當年 表 倪雅玲 負責人 部門 亞洲 基金會 研究 戰略 法國智庫 實驗室 監控 研究員 中方 培訓 法方 原本 計畫 合作 國 兩 實驗室 徹底 法國人 揭幕典禮 盛大 實驗室 武漢 法國人 開放 民主思想 進化 政權 以為 外交 象 形 對外 官方 中國 法國 世界貿易組織 中國 正值 當時 堅決 細菌武器 製造 中共 擔心 外交部 對外安全總局 法國 法國國防部 傳染病 抵抗 中國 疫情 症候群 呼吸道 急性 幫助 主張 科學家 政壇 技術 中國 倡議 哈發林 總理 哈克 席 法國總統 實驗室 安全 生物 國家 武漢 學院 科 中國 全名是 實驗室 武漢 調查報告 實驗室 武漢 針對 塞爾 拉 專家 戰略 防務 日刊 報 費加洛 法國 電台 國際廣播 法國 正常國家 民主 成 變 進化 中共政權 以為 實驗室 徹底 法方 交易 實驗室 兩國 法國媒體 冠狀病毒 洩 外 實驗室 武漢 中法 日電 台北 中央社 真面目 中共 法國 實驗室 武漢 媒 法 儒 楊昇 帆 曹宇 編輯 中共 法國 實驗室 武漢 媒 法']"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"Tp5YirXfQH8L","executionInfo":{"status":"ok","timestamp":1604578761742,"user_tz":-480,"elapsed":89849,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}},"outputId":"dee2abc3-39c7-4bdd-da42-99f6c5fafe6c","colab":{"base_uri":"https://localhost:8080/"}},"source":["totalWordCountRDD = rdd.flatMap(lambda line: line.split(\" \")) \\\n","             .map(lambda word: (word, 1)) \\\n","             .reduceByKey(lambda a, b: a + b) \\\n","             .sortBy(lambda x: x[1], ascending=False) \\\n","             .map(lambda s: s[0]+' '+str(s[1]))\n","\n","totalWordCountRDD.take(10)"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[' 876237',\n"," '一個 356804',\n"," '地區 137325',\n"," '中國 117048',\n"," '開始 116925',\n"," '人口 113140',\n"," '到 109259',\n"," '進行 107610',\n"," '美國 104387',\n"," '一 97878']"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"R9njAa3hQo6P","executionInfo":{"status":"ok","timestamp":1604578791530,"user_tz":-480,"elapsed":13468,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}}},"source":["totalWordCountRDD.repartition(1).saveAsTextFile(\"totalWordCount_folder\")"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylO-OmM1Qq3B","executionInfo":{"status":"ok","timestamp":1604578867340,"user_tz":-480,"elapsed":807,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}},"outputId":"76c0404a-2e38-447a-8f4e-6857b17d2cd5","colab":{"base_uri":"https://localhost:8080/"}},"source":["!head totalWordCount_folder/part-00000\n","\n","!cp totalWordCount_folder/part-00000 /content/gdrive/My\\ Drive/DataMining/DataWithPTT/totalWordCount.txt"],"execution_count":37,"outputs":[{"output_type":"stream","text":[" 876237\n","一個 356804\n","地區 137325\n","中國 117048\n","開始 116925\n","人口 113140\n","到 109259\n","進行 107610\n","美國 104387\n","一 97878\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W-NrLymhWn-A","executionInfo":{"status":"ok","timestamp":1604578882145,"user_tz":-480,"elapsed":703,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}},"outputId":"268ae604-7f06-41a6-cc6c-9e25da1300b6","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls -l /content/gdrive/My\\ Drive/DataMining/DataWithPTT/"],"execution_count":39,"outputs":[{"output_type":"stream","text":["total 634486\n","-rw------- 1 root root 467225718 Nov  5 11:41 IDTitle_content_string.txt\n","-rw------- 1 root root  72138565 Nov  5 12:17 new_PTT_IDTitle_content_string.txt\n","-rw------- 1 root root  76510702 Nov  5 11:41 PTT_IDTitle_content_string.txt\n","-rw------- 1 root root  33837681 Nov  5 12:21 totalWordCount.txt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1EdSKpKUaDha"},"source":["# 製作word_entropy_table.pickle"]},{"cell_type":"code","metadata":{"id":"2o7Vd6eeaDLQ","executionInfo":{"status":"ok","timestamp":1604579787716,"user_tz":-480,"elapsed":1347,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}},"outputId":"3cb24bd0-97cc-40e1-c89e-3009403619ad","colab":{"base_uri":"https://localhost:8080/"}},"source":["rdd = sc.textFile(file_dir+\"totalWordCount.txt\")\n","rdd.take(5)"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['一個 356804', '地區 137325', '中國 117048', '開始 116925', '人口 113140']"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"PczX2jNraIZ4","executionInfo":{"status":"ok","timestamp":1604579899941,"user_tz":-480,"elapsed":13018,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}},"outputId":"e44a8282-43e4-4432-8aeb-fb8e07ab781a","colab":{"base_uri":"https://localhost:8080/"}},"source":["import math\n","\n","def calcEntropy(WC):\n","  WCpair = WC.split()\n","  word = WCpair[0]\n","  count = WCpair[1]\n","\n","  # 使用總文章數作為除數，我懶得研究怎麼把參數傳進來\n","  # 總文章數為876237 + 57868 = 934105\n","  entropy = -1 * math.log(int(count) / 934105, 2)\n","  return {word:entropy}\n","\n","def mergeEntropyDicts(dictA, dictB):\n","  for key in dictB:\n","    # 不會有重複的key，只要直接複製過來即可\n","    dictA[key] = dictB[key]\n","  return dictA\n","\n","entropyDict = rdd.map(calcEntropy) \\\n","         .reduce(mergeEntropyDicts)\n","\n","print(entropyDict['一個'])\n","print(entropyDict['劉備'])\n","print(entropyDict['張飛'])"],"execution_count":51,"outputs":[{"output_type":"stream","text":["1.3884529392843121\n","10.011451220629336\n","12.390281706751175\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X0PSiwAdaKLC","executionInfo":{"status":"ok","timestamp":1604579932800,"user_tz":-480,"elapsed":1723,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}}},"source":["import pickle\n","\n","with open('word_entropy_table.pickle', 'wb') as f:\n","  pickle.dump(entropyDict, f)"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"AyfEjVW8ahs5","executionInfo":{"status":"ok","timestamp":1604579934864,"user_tz":-480,"elapsed":1416,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}},"outputId":"56ab8f81-f9de-49e2-b99e-0d7e7000f686","colab":{"base_uri":"https://localhost:8080/"}},"source":["!cp word_entropy_table.pickle /content/gdrive/My\\ Drive/DataMining/DataWithPTT\n","\n","!ls -l /content/gdrive/My\\ Drive/DataMining/DataWithPTT/"],"execution_count":53,"outputs":[{"output_type":"stream","text":["total 1014429\n","-rw------- 1 root root 467225718 Nov  5 11:41 IDTitle_content_string.txt\n","-rw------- 1 root root  72138565 Nov  5 12:17 new_PTT_IDTitle_content_string.txt\n","-rw------- 1 root root  76510702 Nov  5 11:41 PTT_IDTitle_content_string.txt\n","-rw------- 1 root root  33837673 Nov  5 12:22 totalWordCount.txt\n","-rw------- 1 root root  76714097 Nov  5 12:38 word_entropy_table.pickle\n","-rw------- 1 root root 312346412 Nov  5 12:32 word_ID_table.pickle\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OxXchM3-SN7_"},"source":["# 準備製作word_ID_table.pickle"]},{"cell_type":"code","metadata":{"id":"_PvKeGdkTESr","executionInfo":{"status":"ok","timestamp":1604578977425,"user_tz":-480,"elapsed":564,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}},"outputId":"98d2b50e-8926-4070-dafc-e37ef8470a37","colab":{"base_uri":"https://localhost:8080/"}},"source":["rdd = sc.union([sc.textFile(file_dir + \"new_PTT_IDTitle_content_string.txt\"), sc.textFile(file_dir + \"IDTitle_content_string.txt\")])\n","rdd.take(2)"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1588950762 華視新聞 無極限 說謊 規定 條 法 主動 訊息 隻手遮天 選委會 地方 連頭 行動 韓 為了 主委 中選會 更何況 法條 辦理 規定 日 投票 員工 區內 選舉 本來 投票權 區 選舉 戶籍 規定 常識 知識 高雄市議員 身 黃捷 孫大千 選民 民意代表 樣 聲量 韓 假消息 利用 怒轟黃捷 職守 民意代表 做 功夫 訊息 民眾 高雄市議員 身 黃捷 批評 發文 臉書 下午 孫大千 立委 此事 針對 黃捷 批 怒 書 臉 孫大千 辦理 規定 勞動部 事項 工資 勞工 勞基法 理 辦 規定 日 投票 員工 區內 選舉 投票權 區 選舉 戶籍 日均 投票 前往 員工 機構 事業 團體 學校 機關 各級 區內 選舉 公告 中選會 一致 打電話 每個人 國定假日 即為 當天 高雄市 設籍 民眾 人員 選委會 選委會 高雄市 網 華視新聞 國定假日 高雄 設籍 民眾 選委會 高雄 消息 選委會 高雄市 黃捷怒 範疇 日 投票 選舉 地區性 這類 選舉 選舉 有分 選舉 選委會 高雄市 沒想 投票 回來 上班 需 當天 國定假日 當天 日 投票 詢問 選委會 高雄市 陳情 民眾 高雄 戶籍 工作 縣市 發文 臉書 今在 黃捷 範圍 假 日 投票 韓 選委會 高雄市 爆料 黃捷 道歉 公開 黃捷 聲量 韓 假消息 利用 黃捷 怒 孫大千 立委 國定假日 投票 新聞稿 火速 中選會 消息 選委會 高雄市 範疇 投票 區域 投票 韓 登場 下月 說 選委會 高雄市 陳情 民眾 上午 日 今 黃捷 高雄市議員 時代力量 高雄市 綜合 庭 徐筠 聲量 韓 假消息 利用 怒轟 孫大千 黃捷 假消息 利用 怒轟 孫大千 黃捷',\n"," '1588950829 短 道路 國家利益 維護 條 法國 澳洲 日本 歐洲國家 美國 國家 經貿關係 優先發展 國家 規範 體系 價值觀 本能 外交政策 法國 說 外交官 名 法國 中國 戰是 狀況 法國 改變 危險關係 病毒 肺炎 武漢 實驗室 武漢 所有 中國 技 國家 價值觀 尊重 經濟 問責 中國 說 法國外交官 法國 核武 研發 他國 難免會 理論 元素 鈽 基礎設施 用途 軍事 嚴格來說 工廠 核 迎頭趕上 中國 合理性 技術 中國 法國外交官 國家 沿線 一帶一路 出口 地方 國 電廠 低成本 希望 中國 專家 方案 這項 延宕 雙邊 法 立場 法國外交部 風險 專家 項目 兩用 軍民 核廢料 計畫 合作 歐元 達 規模 雖是 方案 工廠 核 中國 歐安 公司 法國核能 說 前車之鑑 實驗室 合作 技術 知識 改革開放 藉由 國家 工廠 世界 中國 企圖 技術 西方 手段 藉 中國 規 法 合作 實驗室 軍事 民用 並於 中央軍民融合發展委員會 領導 平 國家主席 中國 藩籬 之間 技術 和民 軍用 正式 中國 能力 關鍵 中國 空中巴士 考量 財務 工廠 鏡子 製造 中國人 當時 表示遺憾 技術 掠奪 中國 利益 經濟 短期 空中巴士 巨擘 歐洲航空 人士 法籍 北京 用途 軍事 單位 研究 類型 北京 懷疑 法國 說 中國 實驗室 武漢 複製技術 出口 技術 外國 複製 全國 技術 外國 消化 技術 外國 面 張貼 入口處 核電廠 反應爐 壓水式 國 法 核電廠 台山 中國 說 中國 移轉 技術 棘手 法國 令 技術 兩用 軍民 出售 中華人民共和國 共產黨 為何 問題 法國 目前 爆發 疫情 肺炎 武漢 險境 法國 反使 技術 這種 國家 無止境 發展 軍力 工具 生化武器 研發 中國 日後 複製 抄襲 中國人 說 法國外交官 名 不具 用途 軍事 單位 研究 類型 北京 懷疑 法國 說 中國 實驗室 武漢 歷史 冠狀病毒 重寫 日期 研究 國家 黨 以來 疫情 這次 領導 共產黨 獨立 科學 國家 國家 列寧主義 中國 忘了 倪雅玲 正常國家 中國 開放 資本主義 人員 研究 現實 中國制度 科學界 整個 當年 表 倪雅玲 負責人 部門 亞洲 基金會 研究 戰略 法國智庫 實驗室 監控 研究員 中方 培訓 法方 原本 計畫 合作 國 兩 實驗室 徹底 法國人 揭幕典禮 盛大 實驗室 武漢 法國人 開放 民主思想 進化 政權 以為 外交 象 形 對外 官方 中國 法國 世界貿易組織 中國 正值 當時 堅決 細菌武器 製造 中共 擔心 外交部 對外安全總局 法國 法國國防部 傳染病 抵抗 中國 疫情 症候群 呼吸道 急性 幫助 主張 科學家 政壇 技術 中國 倡議 哈發林 總理 哈克 席 法國總統 實驗室 安全 生物 國家 武漢 學院 科 中國 全名是 實驗室 武漢 調查報告 實驗室 武漢 針對 塞爾 拉 專家 戰略 防務 日刊 報 費加洛 法國 電台 國際廣播 法國 正常國家 民主 成 變 進化 中共政權 以為 實驗室 徹底 法方 交易 實驗室 兩國 法國媒體 冠狀病毒 洩 外 實驗室 武漢 中法 日電 台北 中央社 真面目 中共 法國 實驗室 武漢 媒 法 儒 楊昇 帆 曹宇 編輯 中共 法國 實驗室 武漢 媒 法']"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"id":"GkZ3-6ePSNVG","executionInfo":{"status":"ok","timestamp":1604578983329,"user_tz":-480,"elapsed":547,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}}},"source":["def reverseLink(line):\n","  words = line.split()\n","  # words[0] 格式為 ID:TITLE\n","  # words[1:] 為 各個相關的字\n","  wordID = words[0]\n","  words.pop(0)\n","  \n","  retDict = {}\n","  for word in words:\n","    retDict[word] = [wordID]\n","  \n","  # retDict的格式： retDict[string] = [list with ID]\n","  return retDict\n","\n","def mergeDicts(dictA, dictB):\n","  for key in dictB:\n","    if key in dictA:\n","      # dict[key] is a list with ID.\n","      # [*[1, 2], *[3, 4]] -> [1, 2, 3, 4]\n","      dictA[key] = [*dictA[key], *dictB[key]]\n","    else:\n","      # 只有B有的資料，直接複製到A\n","      dictA[key] = dictB[key]\n","  return dictA"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"q46f9bZ1SayY","executionInfo":{"status":"ok","timestamp":1604579404006,"user_tz":-480,"elapsed":412939,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}},"outputId":"49106c82-2f2a-42cf-c0b5-c3df22065348","colab":{"base_uri":"https://localhost:8080/"}},"source":["word_ID_table = rdd.map(reverseLink) \\\n","         .reduce(mergeDicts)\n","# 回傳了一個dictionary，即：word_ID_table為一個hash map，string->ID_list\n","for k in word_ID_table:\n","  print(k)\n","  break"],"execution_count":42,"outputs":[{"output_type":"stream","text":["華視新聞\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PEuHoKkBScQx","executionInfo":{"status":"ok","timestamp":1604579456554,"user_tz":-480,"elapsed":633,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}},"outputId":"801611a2-e259-4f7f-ca37-110896b33764","colab":{"base_uri":"https://localhost:8080/"}},"source":["word_ID_table['華視新聞']"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1588950762',\n"," '1589003461',\n"," '1589091111',\n"," '1589111478',\n"," '1589199791',\n"," '1589211139',\n"," '1589213441',\n"," '1589289017',\n"," '1589342815',\n"," '1589372058',\n"," '1589380321',\n"," '1589525574',\n"," '1589712553',\n"," '1589786382',\n"," '1589963770',\n"," '1590069129',\n"," '1590072622',\n"," '1590085887',\n"," '1590123867',\n"," '1590154073',\n"," '1590236836',\n"," '1590325634',\n"," '1590380316',\n"," '1590503949',\n"," '1590567280',\n"," '1590650949',\n"," '1590671372',\n"," '1590680128',\n"," '1590819172',\n"," '1590849225',\n"," '1590899903',\n"," '1590935483',\n"," '1591004185',\n"," '1591085968',\n"," '1591088851',\n"," '1591102583',\n"," '1591108222',\n"," '1591114708',\n"," '1591231693',\n"," '1591247554',\n"," '1591282915',\n"," '1591319856',\n"," '1591330240',\n"," '1591514342',\n"," '1591586739',\n"," '1591595203',\n"," '1591679483',\n"," '1591703997',\n"," '1591761517',\n"," '1591846247',\n"," '1591861842',\n"," '1591868238',\n"," '1591886189',\n"," '1592150057',\n"," '1592292401',\n"," '1592469674',\n"," '1592728035',\n"," '1592987713',\n"," '1593131240',\n"," '1593133114',\n"," '1593163635',\n"," '1593186880',\n"," '1593247359',\n"," '1593262708',\n"," '1593349606',\n"," '1591514342',\n"," '1591586739',\n"," '1591595203',\n"," '1591679483',\n"," '1591703997',\n"," '1591761517',\n"," '1591861842',\n"," '1591886189',\n"," '1592469674',\n"," '1592728035',\n"," '1593131240',\n"," '1593133114',\n"," '1593262708',\n"," '1593349606',\n"," '1593496795',\n"," '1593588814',\n"," '1593690340',\n"," '1593766938',\n"," '1593833485',\n"," '1594023081',\n"," '1594037479',\n"," '1594196562',\n"," '1594385099',\n"," '1594456338',\n"," '1594578217',\n"," '1594624352',\n"," '1594644793',\n"," '1594740463',\n"," '1594877518',\n"," '1594945450',\n"," '1594949488',\n"," '1594990332',\n"," '1595079039',\n"," '1595156270',\n"," '1595343394',\n"," '1595423298',\n"," '1595478610',\n"," '1595666350',\n"," '1595823649',\n"," '1595827589',\n"," '1596026116',\n"," '1596104627',\n"," '1596176062',\n"," '1596425733',\n"," '1596549313',\n"," '1596586198',\n"," '1596688357',\n"," '1596704382',\n"," '1596803223',\n"," '1597130415',\n"," '1597223114',\n"," '1597310866',\n"," '1597312977',\n"," '1597401980',\n"," '1597675325',\n"," '1597681682',\n"," '1597758476',\n"," '1597816904',\n"," '1597916634',\n"," '1598156740',\n"," '1598243230',\n"," '1598250204',\n"," '1598257033',\n"," '1598334544',\n"," '1598351813',\n"," '1598370191',\n"," '1598420992',\n"," '1598457347',\n"," '1598503713',\n"," '1598801016',\n"," '1599029163',\n"," '1599062412',\n"," '1599079410',\n"," '1599126232',\n"," '1599243491',\n"," '1599293581',\n"," '1599438530',\n"," '1599544659',\n"," '1599712451',\n"," '1599825555',\n"," '1599835077',\n"," '1599888918',\n"," '1599890357',\n"," '1599908339',\n"," '1600698072',\n"," '1600943090',\n"," '1600982791',\n"," '1601006044',\n"," '1601017887',\n"," '1601465517',\n"," '1601643086',\n"," '1601643322',\n"," '1601992940',\n"," '1602079699',\n"," '1602146256',\n"," '1602218399',\n"," '1602243338',\n"," '1602339930',\n"," '1602518256',\n"," '1602580424',\n"," '1602599040',\n"," '1602599048',\n"," '1602802509',\n"," '1602850203',\n"," '1603088140',\n"," '1603106331',\n"," '1603180291',\n"," '1603285615',\n"," '1603298718',\n"," '1603376969',\n"," '1603382549',\n"," '1603419382',\n"," '1603613406',\n"," '1603623024',\n"," '1603692023',\n"," '1603805135',\n"," '1603805245',\n"," '1603894233',\n"," '1603895656',\n"," '1604033386',\n"," '1604067867',\n"," '19798',\n"," '118290',\n"," '272057',\n"," '341738',\n"," '373363',\n"," '373804',\n"," '418522',\n"," '520755',\n"," '577956',\n"," '756953',\n"," '974465',\n"," '975596',\n"," '1216058',\n"," '1241747',\n"," '1342386',\n"," '1355130',\n"," '1768181',\n"," '3200359',\n"," '3915292',\n"," '4070145',\n"," '4785842']"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"wuSPDqoXSd4g","executionInfo":{"status":"ok","timestamp":1604579471804,"user_tz":-480,"elapsed":10056,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}}},"source":["import pickle\n","\n","with open('word_ID_table.pickle', 'wb') as f:\n","  pickle.dump(word_ID_table, f)"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Wf-2FauY4o6","executionInfo":{"status":"ok","timestamp":1604579522137,"user_tz":-480,"elapsed":1984,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}}},"source":["!cp 'word_ID_table.pickle' /content/gdrive/My\\ Drive/DataMining/DataWithPTT/"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"3_I1L9kRZAgg","executionInfo":{"status":"ok","timestamp":1604579531063,"user_tz":-480,"elapsed":819,"user":{"displayName":"thumbd78492","photoUrl":"","userId":"17499780904171105676"}},"outputId":"179e9d23-7e25-46eb-b9ad-7ee39b4a9d91","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls -l /content/gdrive/My\\ Drive/DataMining/DataWithPTT/"],"execution_count":49,"outputs":[{"output_type":"stream","text":["total 939512\n","-rw------- 1 root root 467225718 Nov  5 11:41 IDTitle_content_string.txt\n","-rw------- 1 root root  72138565 Nov  5 12:17 new_PTT_IDTitle_content_string.txt\n","-rw------- 1 root root  76510702 Nov  5 11:41 PTT_IDTitle_content_string.txt\n","-rw------- 1 root root  33837673 Nov  5 12:22 totalWordCount.txt\n","-rw------- 1 root root 312346412 Nov  5 12:32 word_ID_table.pickle\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I9FHhwb1ZJXi"},"source":[""],"execution_count":null,"outputs":[]}]}